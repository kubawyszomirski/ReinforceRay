{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8fc8c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10294.532652920545, 21.991946458165227  (trained on normal env)\n",
    "# 10291.333417544305, 21.18661703314784 \n",
    "# 5866.360775249422, 56.68289779522459 - base policy ( ithink)\n",
    "\n",
    "# 10884.874199650516, 29.484380976179228 - every step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e99d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 steps\n",
    "# with degradation\n",
    "# with failure\n",
    "# no 2 in action\n",
    "# changed order\n",
    "# random start withdegradation (85 to 100)\n",
    "# export cap, decreasing fit\n",
    "# fixed IRR calculation\n",
    "# energy prices and FIT in the obs space (normaliosed)\n",
    "# if replaced the bos is not calculated in \n",
    "# efficency impr incl\n",
    "# CAPEX incl, and in the obs space\n",
    "# TOU included\n",
    "# , simplified, optimised\n",
    "# no npv and irr in the train env\n",
    "# pv costs and accual intwerest sadded to the mix\n",
    "#  not loading data as pd dataframes\n",
    "# changed action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7702da",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kubaw\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import random  \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "from environment_fx import calculate_import_export, test1, test2, test3, evaluate1, evaluate2, basepolicy\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import torch\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f3f975",
   "metadata": {
    "code_folding": [
     0,
     22
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# import and modify data\n",
    "\n",
    "# Assuming the file is a CSV and specifying the correct path and filename\n",
    "file_path = r\"C:\\Users\\kubaw\\Desktop\\DELFT\\THESIS\\CH5\"\n",
    "\n",
    "# Use pandas to read the CSV file\n",
    "AC_OUTPUT = pd.read_csv(file_path + \"/AC_OUTPUT_JA\")\n",
    "elec_df = pd.read_csv(file_path + \"/hourly_consumption_gemany.csv\")\n",
    "import_price = pd.read_csv(file_path + \"/electricity_tariff.csv\")\n",
    "\n",
    "#elec_df = elec_df * 1000\n",
    "elec_df = elec_df.drop('HourOfYear', axis=1)\n",
    "\n",
    "elec_df['hour_of_day'] = np.arange(8760) % 24\n",
    "elec_df['day_of_week'] = np.arange(8760) // 24 % 7  # 0 is Monday, 6 is Sunday\n",
    "\n",
    "# Define rates\n",
    "peak_rate = 1.45\n",
    "normal_rate = 1\n",
    "off_peak_rate = 0.85\n",
    "\n",
    "# Function to determine rate based on hour and day\n",
    "def determine_rate(hour, day):\n",
    "    if day < 5:  # Monday to Friday\n",
    "        if 16 <= hour < 21:  # 4pm to 9pm\n",
    "            return peak_rate\n",
    "        elif 6 <= hour < 10:  # 7am to 9am and 10am to 3pm\n",
    "            return normal_rate\n",
    "        else:  # Off-peak times\n",
    "            return off_peak_rate\n",
    "    else:  # Weekend\n",
    "        if 16 <= hour < 21:  # 4pm to 9pm\n",
    "            return normal_rate\n",
    "        else:  # Off-peak times\n",
    "            return off_peak_rate\n",
    "    \n",
    "# Apply the function to each row to determine the rate\n",
    "elec_df['rate'] = elec_df.apply(lambda row: determine_rate(row['hour_of_day'], row['day_of_week']), axis=1)\n",
    "\n",
    "import_price_df = import_price.drop(columns=['x'])\n",
    "import_price_df = import_price_df[:-26]\n",
    "\n",
    "train_cols = random.sample(list(import_price_df.columns), 7000)\n",
    "import_price_train = import_price_df[train_cols]\n",
    "test_cols = [col for col in import_price_df.columns if col not in train_cols]\n",
    "import_price_test = import_price_df[test_cols]\n",
    "\n",
    "Eff = pd.read_csv(file_path + \"/Efficency_impr\")\n",
    "Eff = (Eff)/100 + 1\n",
    "\n",
    "CAPEX = pd.read_csv(file_path + \"/CAPEX_JA.csv\")\n",
    "CAPEX_JA = (CAPEX[:26]) * 1.3\n",
    "\n",
    "\n",
    "train_cols_CAPEX = random.sample(list(CAPEX_JA.columns), 7000)\n",
    "test_cols_CAPEX = [col for col in CAPEX_JA.columns if col not in train_cols_CAPEX]\n",
    "\n",
    "CAPEX_JA_train = CAPEX_JA[train_cols_CAPEX]\n",
    "CAPEX_JA_test = CAPEX_JA[test_cols_CAPEX]\n",
    "\n",
    "train_cols_Eff = random.sample(list(Eff.columns), 7000)\n",
    "test_cols_Eff = [col for col in Eff.columns if col not in train_cols_Eff]\n",
    "\n",
    "Eff_train = Eff[train_cols_Eff]\n",
    "Eff_test = Eff[test_cols_Eff]\n",
    "\n",
    "AC_OUTPUT_arr = (np.array(AC_OUTPUT.T)).flatten()\n",
    "\n",
    "Eff_train_arr = np.array(Eff_train.T)\n",
    "Eff_test_arr = np.array(Eff_test.T)\n",
    "\n",
    "CAPEX_JA_train_arr = np.array(CAPEX_JA_train.T)\n",
    "CAPEX_JA_test_arr = np.array(CAPEX_JA_test.T)\n",
    "\n",
    "elec_consum_arr = np.array(elec_df[\"Consumption\"])\n",
    "import_price_rate = np.array(elec_df[\"rate\"])\n",
    "\n",
    "import_price_train_arr = np.array(import_price_train.T)\n",
    "import_price_test_arr = np.array(import_price_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01482e1",
   "metadata": {
    "code_folding": [
     0,
     1,
     40,
     206,
     352
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class TrainEnvironment(gym.Env):\n",
    "    def __init__(self, AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_tariff, efficency, CAPEX):\n",
    "        \n",
    "        # Price per watthour\n",
    "        self.import_price_df = import_tariff\n",
    "        self.import_price_at_zero = np.float32(0.00035)\n",
    "        self.import_price_rate = import_price_rate\n",
    "        \n",
    "        # Energy Balance\n",
    "        self.AC_OUTPUT = AC_OUTPUT_arr\n",
    "        self.elec_df = elec_consum_arr\n",
    "        self.max_export = 4000\n",
    "        self.number_of_panels = 24\n",
    "        \n",
    "        # Degradation\n",
    "        self.deg_mu = 1.055\n",
    "        self.deg_std = 0.555\n",
    "        \n",
    "        # Efficency Development\n",
    "        self.efficency_develop_df = efficency\n",
    "        self.efficency_at_zero = 1.0\n",
    "        \n",
    "        # Costs\n",
    "        self.power_at_zero = 415\n",
    "        self.cost_per_Wp_df_at_zero = 0.69\n",
    "        self.cost_per_Wp_df = CAPEX\n",
    "        self.initial_other_costs = 150\n",
    "        \n",
    "        self.operational_cost = 16.8\n",
    "        #self.budget_constraint = 1500\n",
    "        \n",
    "        self.loan_interest_rate = 1.06\n",
    "        self.normal_interest_rate = 1.02\n",
    "                        \n",
    "        # Spaces and length\n",
    "        self.action_space = spaces.Discrete(self.number_of_panels + 1)\n",
    "        self.observation_space = spaces.Box(0, 1.25, shape=(self.number_of_panels + 7,))\n",
    "        self.episode_len = 25\n",
    "        self.months_per_timestep = 12\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        \n",
    "        return self.observation\n",
    "    \n",
    "    def calculate_import_export(self, AC_OUTPUT, elec_df, export_price, import_price):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate the annual Wh of energy exported to the grid (exported) and saved (minimised)\n",
    "        \"\"\"\n",
    "        \n",
    "        AC_OUTPUT_tot = self._get_obs()[0:self.number_of_panels].sum() * self.AC_OUTPUT \n",
    "\n",
    "        exported = (AC_OUTPUT_tot - self.elec_df).clip(min=0, max = self.max_export)        \n",
    "        export_revenue = (export_price * exported).sum()\n",
    "\n",
    "        \n",
    "        minimised = AC_OUTPUT_tot - exported \n",
    "        minimised_revenue = (minimised * (self.import_price_rate * import_price)).sum()\n",
    "        \n",
    "\n",
    "        return export_revenue, AC_OUTPUT_tot, minimised_revenue\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reset the environment to the original state at t=1\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Panels\n",
    "        self.init_obs = np.random.uniform(0, 1, size=self.number_of_panels).astype(np.float32)\n",
    "        self.init_obs = np.where(self.init_obs < 0.5, 0.0, np.random.uniform(0.85, 1.0, size=self.number_of_panels))\n",
    "\n",
    "        # Combine all initialization into a single step for efficiency\n",
    "        self.import_price_at_zero_norm = (self.import_price_at_zero - 0.00022499) / (0.0020798 - 0.00022499)\n",
    "        self.FiT_at_zero_norm = (self.import_price_at_zero - 0.00022499 * 0.33) / (0.0020798 - 0.00022499 * 0.33)\n",
    "        self.efficency_at_zero_norm = (self.efficency_at_zero - 0.999) / (1.156 - 0.999)\n",
    "        self.panel_cost_and_inverter_at_zero_norm = (self.cost_per_Wp_df_at_zero - 0.254) / (1.12 - 0.254)\n",
    "        \n",
    "        self.current_budget_constraint = np.random.randint(0, 750)\n",
    "        self.next_step_budget_constraint = 0\n",
    "        \n",
    "        \n",
    "        # Complete observation initialization in one go\n",
    "        self.observation = np.concatenate([\n",
    "            self.init_obs,\n",
    "            [self.import_price_at_zero_norm, self.FiT_at_zero_norm, self.efficency_at_zero_norm, \n",
    "             self.panel_cost_and_inverter_at_zero_norm, 0., 0., 0.]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        self.previous_observation = self.observation.copy()\n",
    "\n",
    "        # RANDOM IMPORT PRICE\n",
    "        self.random_import_price = self.import_price_df[np.random.choice(self.import_price_df.shape[0])] \n",
    "\n",
    "        # RANDOM EFFICENCY\n",
    "        self.random_efficency_develop = self.efficency_develop_df[np.random.choice(self.efficency_develop_df.shape[0])]   \n",
    "        \n",
    "        # RANDOM COST PER WP\n",
    "        self.random_cost_per_Wp = self.cost_per_Wp_df[np.random.choice(self.cost_per_Wp_df.shape[0])]   \n",
    "        \n",
    "        \n",
    "        self.episode_len = 25  \n",
    "    \n",
    "        info = {}\n",
    "        \n",
    "        # RESET BALANCES\n",
    "        self.fin_balance_tot = 0\n",
    "        self.reward_tot = 0\n",
    "        self.env_balance_tot = 0\n",
    "        self.produced = 0\n",
    "        self.other_costs = 0\n",
    "        self.FiT = 0.0004\n",
    "        self.next_FiT = 0.0004\n",
    "\n",
    "        self.total_cash_flow = []\n",
    "        self.annual_cash_flow = 0\n",
    "                \n",
    "        self.due_loans = [0, 0, 0, 0] \n",
    "        self.current_interest = 0\n",
    "        self.step_total_interest = 1\n",
    "        self.survival = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        self.resale_values = array_of_zeros = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        \n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        \n",
    "        self.two_year_ago_interest = 0\n",
    "        self.first_year_interest = []\n",
    "        self.second_year_interest = [0]\n",
    "        self.third_year_interest = [0, 0]\n",
    "        self.fourth_year_interest = [0, 0, 0]\n",
    "        self.next_year_total = 0\n",
    "        \n",
    "        self.survival = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "    \n",
    "        return self.observation, info\n",
    "    \n",
    "    def calculate_resale(self, initial_panel_cost, indices):\n",
    "        \n",
    "        self.resale_values[indices] = initial_panel_cost\n",
    "        \n",
    "        self.resale_values = self.resale_values * 0.85\n",
    "        \n",
    "        for count, i in enumerate(self.broke):\n",
    "            if i == 1:\n",
    "                self.resale_values[count] = 0\n",
    "        \n",
    "        resale_step = self.resale_values[indices].sum()\n",
    "        \n",
    "        return resale_step\n",
    "    \n",
    "    def calculate_panel_inv_cost(self, cost_per_Wp):\n",
    "        \n",
    "        PW_ep = self.efficency_develop * self.power_at_zero\n",
    "        \n",
    "        panel_cost_and_inverter = PW_ep * cost_per_Wp\n",
    "        \n",
    "        return panel_cost_and_inverter\n",
    "        \n",
    "    def calculate_penalty(self, current_step, annual_expense):\n",
    "              \n",
    "        year = 25 - current_step\n",
    "        \n",
    "        if year > 0:\n",
    "            self.current_budget_constraint = self.next_step_budget_constraint    \n",
    "            \n",
    "        \n",
    "        self.current_interest = self.next_year_total\n",
    "        annual_expense = (-annual_expense)\n",
    "        value = 0 \n",
    "        loan = 0\n",
    "        annual_interest = 0\n",
    "\n",
    "        if annual_expense > self.current_budget_constraint:\n",
    "            loan = (self.current_budget_constraint - annual_expense)\n",
    "            value = annual_expense / self.current_budget_constraint\n",
    "            periods = 2 if value < 2 else 3 if value < 3 else 4\n",
    "\n",
    "            annual_interest = loan / periods\n",
    "            interest_multiplier = 1\n",
    "\n",
    "            for i in range(4):\n",
    "                if i < periods:\n",
    "                    self.due_loans[i] = annual_interest * interest_multiplier\n",
    "                    interest_multiplier *= self.loan_interest_rate\n",
    "                else:\n",
    "                    self.due_loans[i] = 0\n",
    "        else:\n",
    "             self.due_loans = [0, 0, 0, 0]\n",
    "    \n",
    "        self.first_year_interest.append(self.due_loans[0])\n",
    "        self.second_year_interest.append(self.due_loans[1])\n",
    "        self.third_year_interest.append(self.due_loans[2])\n",
    "        self.fourth_year_interest.append(self.due_loans[3])\n",
    "    \n",
    "    \n",
    "        self.next_year_total = self.first_year_interest[year] + self.second_year_interest[year] + self.third_year_interest[year] + self.fourth_year_interest[year]\n",
    "        \n",
    "        self.next_step_budget_constraint = np.random.randint(0, 750) * self.step_total_interest\n",
    "        current_budget_observation = (self.next_step_budget_constraint - 0 * self.step_total_interest) / (750 * self.step_total_interest - 0 * self.step_total_interest) \n",
    "        self.observation[self.number_of_panels + 6] = current_budget_observation\n",
    "                \n",
    "        return self.current_interest, self.due_loans, self.next_year_total\n",
    "        \n",
    "    def calculate_total_CAPEX(self, action_step, panel_cost_and_inverter):\n",
    "        \"\"\"\n",
    "        Calculate CAPEX each step in a vectorized manner.\n",
    "        \"\"\"\n",
    "        BOS = panel_cost_and_inverter * 0.55\n",
    "        number_installed = int(np.sum(action_step))\n",
    "\n",
    "        # Calculate costs from module and inverter\n",
    "        panel_cost_and_inverter_step = panel_cost_and_inverter * number_installed\n",
    "\n",
    "        # Calculate other installation costs\n",
    "        if number_installed == 0:\n",
    "            other_costs = 0\n",
    "        elif number_installed == 1:\n",
    "            other_costs = self.initial_other_costs * self.step_total_interest\n",
    "        else:\n",
    "            discounts = 0.9 ** np.arange(number_installed)\n",
    "            other_costs = (self.initial_other_costs * self.step_total_interest * discounts).sum()\n",
    "\n",
    "        # Calculate BOS costs using vector operations\n",
    "        is_new_installation = (self.previous_observation[:number_installed] == 0) & (action_step[:number_installed] == 1)\n",
    "        is_replacement = (self.previous_observation[:number_installed] > 0) & (action_step[:number_installed] == 1)\n",
    "        BOS_cost = np.sum(BOS * is_new_installation) + np.sum((BOS / 2) * is_replacement)\n",
    "\n",
    "        # Sum total CAPEX\n",
    "        total_CAPEX = panel_cost_and_inverter_step + BOS_cost + other_costs\n",
    "\n",
    "        return total_CAPEX, panel_cost_and_inverter\n",
    "        \n",
    "    def failure(self, actions):\n",
    "        \n",
    "        beta = 3  # Shape parameter\n",
    "        phi = 30  # Scale parameter  \n",
    "\n",
    "        # Determine which panels are active based on the actions and previous observations.\n",
    "        if self.episode_len == 24:\n",
    "            active_panels = (self.observation[:self.number_of_panels] > 0.85)\n",
    "        else:\n",
    "            active_panels = (self.observation[:self.number_of_panels] == self.efficency_develop)\n",
    "\n",
    "        # Calculate lifespan for all active panels at once\n",
    "        lifespans = np.random.weibull(beta, self.number_of_panels) * phi\n",
    "        lifespans = np.where(active_panels, lifespans, 0)  # Apply lifespan only to active panels\n",
    "\n",
    "        # Adjust survival times based on episode length\n",
    "        self.survival[:self.number_of_panels] = np.where(\n",
    "            active_panels,\n",
    "            np.abs(lifespans.astype(int)) + np.abs(self.episode_len - 25),\n",
    "            self.survival[:self.number_of_panels]\n",
    "        )\n",
    "\n",
    "        return self.survival\n",
    "\n",
    "    def calculate_FiT(self, episodes, import_price):\n",
    "            \n",
    "        self.FiT = import_price\n",
    "            \n",
    "        if episodes == 25:\n",
    "            self.FiT = self.FiT\n",
    "            \n",
    "        elif episodes == 24 or episodes == 23:\n",
    "            self.FiT = self.FiT * 0.64\n",
    "            \n",
    "        elif episodes == 22:\n",
    "            self.FiT = self.FiT * 0.46\n",
    "            \n",
    "        elif episodes == 21:\n",
    "            self.FiT = self.FiT * 0.55\n",
    "            \n",
    "        elif episodes < 20:\n",
    "            self.FiT = self.FiT * 0.33\n",
    "            \n",
    "        elif episodes == 20:\n",
    "            self.FiT = self.FiT * 0.37\n",
    "            \n",
    "        return self.FiT\n",
    "                        \n",
    "    def step(self, action):\n",
    "        \n",
    "        \"\"\"\n",
    "        defines actions, reward etc.\n",
    "        \"\"\"\n",
    "        \n",
    "        # RESET THE ANNUAL BALANCES\n",
    "        self.total_CAPEX = 0\n",
    "        self.pv_costs = 0\n",
    "        self.fin_balance = 0\n",
    "        self.number_installed = 0\n",
    "        current_penalty = 0\n",
    "        self.other_costs = 0\n",
    "        next_step_penalty = 0\n",
    "        self.step_total_interest = self.step_total_interest * self.normal_interest_rate\n",
    "        current_operational_costs = self.operational_cost * self.step_total_interest\n",
    "        \n",
    "        \n",
    "        self.cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "        self.import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "        self.efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "           \n",
    "        self.panel_cost_and_inverter = self.calculate_panel_inv_cost(self.cost_per_Wp)\n",
    "        FiT = self.calculate_FiT(self.episode_len, self.import_price)\n",
    "        \n",
    "        reward = 0   \n",
    "        actions_step = np.random.rand(8)\n",
    "        \n",
    "        # Find indices of the lowest 'action' values in previous_observation\n",
    "        indices = np.argsort(self.previous_observation[:self.number_of_panels])[:action]\n",
    "\n",
    "        # Replace these indices in the observation with efficiency_develop\n",
    "        self.observation[:self.number_of_panels][indices] = self.efficency_develop\n",
    "\n",
    "        # Copy over the other values from previous_observation to observation\n",
    "        mask = np.ones(len(self.previous_observation[:self.number_of_panels]), dtype=bool)\n",
    "        mask[indices] = False\n",
    "        self.observation[:self.number_of_panels][mask] = self.previous_observation[:self.number_of_panels][mask]\n",
    "\n",
    "        replaced_panels = np.zeros(len(self.previous_observation[:self.number_of_panels]), dtype=int)\n",
    "        replaced_panels[indices] = 1\n",
    "\n",
    "        instaltion = (self.observation[:self.number_of_panels] > 0).astype(int)\n",
    "        self.pv_costs -= instaltion.sum() * current_operational_costs\n",
    "\n",
    "        actions_step = np.array(replaced_panels)\n",
    "\n",
    "            \n",
    "        if action > 0:\n",
    "            step_CAPEX, panel_cost_and_inverter = self.calculate_total_CAPEX(actions_step, self.panel_cost_and_inverter)\n",
    "            self.pv_costs -= step_CAPEX\n",
    "            \n",
    "        else:\n",
    "            panel_cost_and_inverter = 0\n",
    "                \n",
    "        next_observation = self._get_obs()\n",
    "\n",
    "        \n",
    "        # Calculate the Reslae value\n",
    "        resale = self.calculate_resale(panel_cost_and_inverter, indices) #  ***\n",
    "        \n",
    "        self.pv_costs += resale\n",
    " \n",
    "        \n",
    "        # CALCULATE THE BUDGET INTEREST\n",
    "        current_penalty, due_loans, next_step_penalty = self.calculate_penalty(self.episode_len, self.pv_costs)\n",
    "\n",
    "        \n",
    "        # CALCULATE THE ENERGY YIELD\n",
    "        exported_revenue, AC_OUTPUT_tot, minimised_revenue = self.calculate_import_export(self.AC_OUTPUT, \n",
    "                                                                          self.elec_df, FiT, self.import_price)        \n",
    "        \n",
    "        pv_costs_observation = - self.pv_costs / 10000\n",
    "        self.observation[self.number_of_panels + 4] = pv_costs_observation\n",
    "        \n",
    "        next_step_penalty_observation = - next_step_penalty / 8000\n",
    "        self.observation[self.number_of_panels + 5] = next_step_penalty_observation\n",
    "        \n",
    "        \n",
    "        # CALCULATE STEP BALANCES\n",
    "        self.fin_balance += self.pv_costs\n",
    "        self.fin_balance += current_penalty\n",
    "        self.fin_balance += float(exported_revenue + minimised_revenue)\n",
    "        \n",
    "        # CALCULATE TOTAL BALANCES\n",
    "        self.fin_balance_tot += self.fin_balance                \n",
    "        \n",
    "        # SUBSTRACT 1 FOR TIMESTEP\n",
    "        self.episode_len -= 1\n",
    "        done = self.episode_len <= 0\n",
    "        \n",
    "        #reward = self.fin_balance_tot / 1000 if done else 0\n",
    "        reward = self.fin_balance / 1000\n",
    "        \n",
    "        # FAILURE\n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        survival = self.failure(actions_step)\n",
    "        \n",
    "        for c, p in enumerate(survival):\n",
    "            \n",
    "            if c < self.number_of_panels:\n",
    "\n",
    "                if p - 1 <= abs(self.episode_len - 24):\n",
    "                    self.broke[c] = 1\n",
    "                    self.observation[c] = 0\n",
    "        \n",
    "        # DEGRADATION RATE\n",
    "        # Applying degradation only to panels that are operational (above 0.1 efficiency)\n",
    "        active_panels = self.observation[:self.number_of_panels] > 0.1\n",
    "        degradations = np.random.normal(self.deg_mu, self.deg_std, size=self.number_of_panels) / 100\n",
    "        self.observation[:self.number_of_panels][active_panels] -= degradations[active_panels]\n",
    "        \n",
    "        if not done: \n",
    "        \n",
    "            self.next_cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "            self.next_import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "            self.next_efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "            next_FIT = self.calculate_FiT(self.episode_len, self.next_import_price)\n",
    "        \n",
    "            price_observation = (self.next_import_price - 0.00022499) / (0.0020798 - 0.00022499)\n",
    "            self.observation[self.number_of_panels] = price_observation\n",
    "\n",
    "            FiT_observation = (next_FIT - 0.00022499 * 0.33) / (0.0020798 - 0.00022499 * 0.33)\n",
    "            self.observation[self.number_of_panels + 1] = FiT_observation\n",
    "\n",
    "            eff_observation = (self.next_efficency_develop - 0.999) / (1.156 - 0.999)\n",
    "            self.observation[self.number_of_panels + 2] = eff_observation\n",
    "\n",
    "            cost_per_Wp_observation = (self.next_cost_per_Wp - 0.254) / (1.12 - 0.254) \n",
    "            self.observation[self.number_of_panels + 3] = cost_per_Wp_observation\n",
    "        \n",
    "        info = {\"step financial balance (eur):\": self.fin_balance,\n",
    "               \"total financial balance: (eur)\": self.fin_balance_tot,\n",
    "               \"internal rate of return\": 0,\n",
    "               \"current_interest\": resale,\n",
    "                \"net present value\": 0}\n",
    "         \n",
    "        \n",
    "        self.previous_observation = self.observation.copy()\n",
    "        \n",
    "        return self.observation, reward, done, False, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff95454",
   "metadata": {
    "code_folding": [
     0,
     40,
     62,
     159,
     225,
     254,
     417
    ]
   },
   "outputs": [],
   "source": [
    "class TestEnvironment(gym.Env):\n",
    "    def __init__(self, AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_tariff, efficency, CAPEX):\n",
    "        \n",
    "        # Price per watthour\n",
    "        self.import_price_df = import_tariff\n",
    "        self.import_price_at_zero = np.float32(0.00035)\n",
    "        self.import_price_rate = import_price_rate\n",
    "        \n",
    "        # Energy Balance\n",
    "        self.AC_OUTPUT = AC_OUTPUT_arr\n",
    "        self.elec_df = elec_consum_arr\n",
    "        self.max_export = 4000\n",
    "        self.number_of_panels = 24\n",
    "        \n",
    "        # Degradation\n",
    "        self.deg_mu = 1.055\n",
    "        self.deg_std = 0.555\n",
    "        \n",
    "        # Efficency Development\n",
    "        self.efficency_develop_df = efficency\n",
    "        self.efficency_at_zero = 1.0\n",
    "        \n",
    "        # Costs\n",
    "        self.power_at_zero = 415\n",
    "        self.cost_per_Wp_df_at_zero = 0.69\n",
    "        self.cost_per_Wp_df = CAPEX\n",
    "        self.initial_other_costs = 150\n",
    "        \n",
    "        self.operational_cost = 16.8\n",
    "        #self.budget_constraint = 1500\n",
    "        \n",
    "        self.loan_interest_rate = 1.06\n",
    "        self.normal_interest_rate = 1.02\n",
    "                        \n",
    "        # Spaces and length\n",
    "        self.action_space = spaces.Discrete(self.number_of_panels + 1)\n",
    "        self.observation_space = spaces.Box(0, 1.25, shape=(self.number_of_panels + 7,))\n",
    "        self.episode_len = 25\n",
    "        self.months_per_timestep = 12\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        \n",
    "        return self.observation\n",
    "    \n",
    "    def calculate_import_export(self, AC_OUTPUT, elec_df, export_price, import_price):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate the annual Wh of energy exported to the grid (exported) and saved (minimised)\n",
    "        \"\"\"\n",
    "        \n",
    "        AC_OUTPUT_tot = self._get_obs()[0:self.number_of_panels].sum() * self.AC_OUTPUT \n",
    "\n",
    "        exported = (AC_OUTPUT_tot - self.elec_df).clip(min=0, max = self.max_export)        \n",
    "        export_revenue = (export_price * exported).sum()\n",
    "\n",
    "        \n",
    "        minimised = AC_OUTPUT_tot - exported \n",
    "        minimised_revenue = (minimised * (self.import_price_rate * import_price)).sum()\n",
    "        \n",
    "\n",
    "        return export_revenue, AC_OUTPUT_tot, minimised_revenue\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reset the environment to the original state at t=1\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Panels\n",
    "        self.init_obs = np.random.uniform(0, 1, size=self.number_of_panels).astype(np.float32)\n",
    "        self.init_obs = np.where(self.init_obs < 0.5, 0.0, np.random.uniform(0.85, 1.0, size=self.number_of_panels))\n",
    "\n",
    "        # Combine all initialization into a single step for efficiency\n",
    "        self.import_price_at_zero_norm = (self.import_price_at_zero - 0.00022499) / (0.0020798 - 0.00022499)\n",
    "        self.FiT_at_zero_norm = (self.import_price_at_zero - 0.00022499 * 0.33) / (0.0020798 - 0.00022499 * 0.33)\n",
    "        self.efficency_at_zero_norm = (self.efficency_at_zero - 0.999) / (1.156 - 0.999)\n",
    "        self.panel_cost_and_inverter_at_zero_norm = (self.cost_per_Wp_df_at_zero - 0.254) / (1.12 - 0.254)\n",
    "        \n",
    "        self.current_budget_constraint = np.random.randint(0, 750)\n",
    "        self.next_step_budget_constraint = 0\n",
    "        \n",
    "        \n",
    "        # Complete observation initialization in one go\n",
    "        self.observation = np.concatenate([\n",
    "            self.init_obs,\n",
    "            [self.import_price_at_zero_norm, self.FiT_at_zero_norm, self.efficency_at_zero_norm, \n",
    "             self.panel_cost_and_inverter_at_zero_norm, 0., 0., 0.]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        self.previous_observation = self.observation.copy()\n",
    "\n",
    "        # RANDOM IMPORT PRICE\n",
    "        self.random_import_price = self.import_price_df[np.random.choice(self.import_price_df.shape[0])] \n",
    "\n",
    "        # RANDOM EFFICENCY\n",
    "        self.random_efficency_develop = self.efficency_develop_df[np.random.choice(self.efficency_develop_df.shape[0])]   \n",
    "        \n",
    "        # RANDOM COST PER WP\n",
    "        self.random_cost_per_Wp = self.cost_per_Wp_df[np.random.choice(self.cost_per_Wp_df.shape[0])]   \n",
    "        \n",
    "        \n",
    "        self.episode_len = 25  \n",
    "    \n",
    "        info = {}\n",
    "        \n",
    "        # RESET BALANCES\n",
    "        self.fin_balance_tot = 0\n",
    "        self.reward_tot = 0\n",
    "        self.env_balance_tot = 0\n",
    "        self.produced = 0\n",
    "        self.other_costs = 0\n",
    "        self.FiT = 0.0004\n",
    "        self.next_FiT = 0.0004\n",
    "        self.resale_values = array_of_zeros = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        \n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        self.total_cash_flow = []\n",
    "        self.annual_cash_flow = 0\n",
    "                \n",
    "        self.due_loans = [0, 0, 0, 0] \n",
    "        self.current_interest = 0\n",
    "        self.step_total_interest = 1\n",
    "        \n",
    "        self.two_year_ago_interest = 0\n",
    "        self.first_year_interest = []\n",
    "        self.second_year_interest = [0]\n",
    "        self.third_year_interest = [0, 0]\n",
    "        self.fourth_year_interest = [0, 0, 0]\n",
    "        self.next_year_total = 0\n",
    "        \n",
    "        self.survival = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "    \n",
    "        return self.observation, info\n",
    "    \n",
    "    def calculate_resale(self, initial_panel_cost, indices):\n",
    "        \n",
    "        self.resale_values[indices] = initial_panel_cost\n",
    "        \n",
    "        self.resale_values = self.resale_values * 0.85\n",
    "        \n",
    "        for count, i in enumerate(self.broke):\n",
    "            if i == 1:\n",
    "                self.resale_values[count] = 0\n",
    "        \n",
    "        resale_step = self.resale_values[indices].sum()\n",
    "        \n",
    "        return resale_step\n",
    "    \n",
    "    def calculate_panel_inv_cost(self, cost_per_Wp):\n",
    "        \n",
    "        PW_ep = self.efficency_develop * self.power_at_zero\n",
    "        \n",
    "        panel_cost_and_inverter = PW_ep * cost_per_Wp\n",
    "        \n",
    "        return panel_cost_and_inverter\n",
    "    \n",
    "    def calculate_irr_and_npv(self, pv_cost, minimised_revenue, export_revenue, penalty):\n",
    "                \n",
    "        \"\"\"\n",
    "        Calculates total cash flow of the project needed for the internal rate of return\n",
    "        \"\"\" \n",
    "        self.expences = 0\n",
    "        self.annual_cash_flow = 0\n",
    "        initial_cost = 0\n",
    "        \n",
    "        self.expences = pv_cost\n",
    "        self.annual_cash_flow = self.expences + export_revenue + minimised_revenue + penalty\n",
    "        initial_cost_q, x = self.calculate_total_CAPEX(self.init_obs, self.panel_cost_and_inverter)\n",
    "        initial_cost = - initial_cost_q\n",
    "        \n",
    "        if self.episode_len == 24:\n",
    "            self.total_cash_flow.append(initial_cost + self.annual_cash_flow) \n",
    "        else:\n",
    "            self.total_cash_flow.append(self.annual_cash_flow) \n",
    "        \n",
    "        return self.total_cash_flow\n",
    "        \n",
    "    def calculate_penalty(self, current_step, annual_expense):\n",
    "              \n",
    "        year = 25 - current_step\n",
    "        \n",
    "        if year > 0:\n",
    "            self.current_budget_constraint = self.next_step_budget_constraint    \n",
    "            \n",
    "        \n",
    "        self.current_interest = self.next_year_total\n",
    "        annual_expense = (-annual_expense)\n",
    "        value = 0 \n",
    "        loan = 0\n",
    "        annual_interest = 0\n",
    "\n",
    "        if annual_expense > self.current_budget_constraint:\n",
    "            loan = (self.current_budget_constraint - annual_expense)\n",
    "            value = annual_expense / self.current_budget_constraint\n",
    "            periods = 2 if value < 2 else 3 if value < 3 else 4\n",
    "\n",
    "            annual_interest = loan / periods\n",
    "            interest_multiplier = 1\n",
    "\n",
    "            for i in range(4):\n",
    "                if i < periods:\n",
    "                    self.due_loans[i] = annual_interest * interest_multiplier\n",
    "                    interest_multiplier *= self.loan_interest_rate\n",
    "                else:\n",
    "                    self.due_loans[i] = 0\n",
    "        else:\n",
    "             self.due_loans = [0, 0, 0, 0]\n",
    "    \n",
    "        self.first_year_interest.append(self.due_loans[0])\n",
    "        self.second_year_interest.append(self.due_loans[1])\n",
    "        self.third_year_interest.append(self.due_loans[2])\n",
    "        self.fourth_year_interest.append(self.due_loans[3])\n",
    "    \n",
    "    \n",
    "        self.next_year_total = self.first_year_interest[year] + self.second_year_interest[year] + self.third_year_interest[year] + self.fourth_year_interest[year]\n",
    "        \n",
    "        self.next_step_budget_constraint = np.random.randint(750, 2000) * self.step_total_interest\n",
    "        current_budget_observation = (self.next_step_budget_constraint - 750 * self.step_total_interest) / (2000 * self.step_total_interest - 750 * self.step_total_interest) \n",
    "        self.observation[self.number_of_panels + 6] = current_budget_observation\n",
    "                \n",
    "        return self.current_interest, self.due_loans, self.next_year_total\n",
    "        \n",
    "    def calculate_total_CAPEX(self, action_step, panel_cost_and_inverter):\n",
    "        \"\"\"\n",
    "        Calculate CAPEX each step in a vectorized manner.\n",
    "        \"\"\"\n",
    "        BOS = panel_cost_and_inverter * 0.55\n",
    "        number_installed = int(np.sum(action_step))\n",
    "\n",
    "        # Calculate costs from module and inverter\n",
    "        panel_cost_and_inverter_step = panel_cost_and_inverter * number_installed\n",
    "\n",
    "        # Calculate other installation costs\n",
    "        if number_installed == 0:\n",
    "            other_costs = 0\n",
    "        elif number_installed == 1:\n",
    "            other_costs = self.initial_other_costs * self.step_total_interest\n",
    "        else:\n",
    "            discounts = 0.9 ** np.arange(number_installed)\n",
    "            other_costs = (self.initial_other_costs * self.step_total_interest * discounts).sum()\n",
    "\n",
    "        # Calculate BOS costs using vector operations\n",
    "        is_new_installation = (self.previous_observation[:number_installed] == 0) & (action_step[:number_installed] == 1)\n",
    "        is_replacement = (self.previous_observation[:number_installed] > 0) & (action_step[:number_installed] == 1)\n",
    "        BOS_cost = np.sum(BOS * is_new_installation) + np.sum((BOS / 2) * is_replacement)\n",
    "\n",
    "        # Sum total CAPEX\n",
    "        total_CAPEX = panel_cost_and_inverter_step + BOS_cost + other_costs\n",
    "\n",
    "        return total_CAPEX, panel_cost_and_inverter\n",
    "        \n",
    "    def failure(self, actions):\n",
    "        \n",
    "        beta = 3  # Shape parameter\n",
    "        phi = 30  # Scale parameter  \n",
    "\n",
    "        # Determine which panels are active based on the actions and previous observations.\n",
    "        if self.episode_len == 24:\n",
    "            active_panels = (self.observation[:self.number_of_panels] > 0.85)\n",
    "        else:\n",
    "            active_panels = (self.observation[:self.number_of_panels] == self.efficency_develop)\n",
    "\n",
    "        # Calculate lifespan for all active panels at once\n",
    "        lifespans = np.random.weibull(beta, self.number_of_panels) * phi\n",
    "        lifespans = np.where(active_panels, lifespans, 0)  # Apply lifespan only to active panels\n",
    "\n",
    "        # Adjust survival times based on episode length\n",
    "        self.survival[:self.number_of_panels] = np.where(\n",
    "            active_panels,\n",
    "            np.abs(lifespans.astype(int)) + np.abs(self.episode_len - 25),\n",
    "            self.survival[:self.number_of_panels]\n",
    "        )\n",
    "\n",
    "        return self.survival\n",
    "\n",
    "    def calculate_FiT(self, episodes, import_price):\n",
    "            \n",
    "        self.FiT = import_price\n",
    "            \n",
    "        if episodes == 25:\n",
    "            self.FiT = self.FiT\n",
    "            \n",
    "        elif episodes == 24 or episodes == 23:\n",
    "            self.FiT = self.FiT * 0.64\n",
    "            \n",
    "        elif episodes == 22:\n",
    "            self.FiT = self.FiT * 0.46\n",
    "            \n",
    "        elif episodes == 21:\n",
    "            self.FiT = self.FiT * 0.55\n",
    "            \n",
    "        elif episodes < 20:\n",
    "            self.FiT = self.FiT * 0.33\n",
    "            \n",
    "        elif episodes == 20:\n",
    "            self.FiT = self.FiT * 0.37\n",
    "            \n",
    "        return self.FiT\n",
    "                        \n",
    "    def step(self, action):\n",
    "        \n",
    "        \"\"\"\n",
    "        defines actions, reward etc.\n",
    "        \"\"\"\n",
    "        \n",
    "        # RESET THE ANNUAL BALANCES\n",
    "        self.total_CAPEX = 0\n",
    "        self.pv_costs = 0\n",
    "        self.fin_balance = 0\n",
    "        self.number_installed = 0\n",
    "        irr_fin = 0\n",
    "        npv_fin = 0\n",
    "        current_penalty = 0\n",
    "        self.other_costs = 0\n",
    "        next_step_penalty = 0\n",
    "        self.step_total_interest = self.step_total_interest * self.normal_interest_rate\n",
    "        current_operational_costs = self.operational_cost * self.step_total_interest\n",
    "        \n",
    "        \n",
    "        self.cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "        self.import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "        self.efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "           \n",
    "        self.panel_cost_and_inverter = self.calculate_panel_inv_cost(self.cost_per_Wp)\n",
    "        FiT = self.calculate_FiT(self.episode_len, self.import_price)\n",
    "        \n",
    "        reward = 0   \n",
    "        actions_step = np.random.rand(self.number_of_panels + 1)\n",
    "        \n",
    "        \n",
    "        # Find indices of the lowest 'action' values in previous_observation\n",
    "        indices = np.argsort(self.previous_observation[:self.number_of_panels])[:action]\n",
    "\n",
    "        # Replace these indices in the observation with efficiency_develop\n",
    "        self.observation[:self.number_of_panels][indices] = self.efficency_develop\n",
    "\n",
    "        # Copy over the other values from previous_observation to observation\n",
    "        mask = np.ones(len(self.previous_observation[:self.number_of_panels]), dtype=bool)\n",
    "        mask[indices] = False\n",
    "        self.observation[:self.number_of_panels][mask] = self.previous_observation[:self.number_of_panels][mask]\n",
    "\n",
    "        replaced_panels = np.zeros(len(self.previous_observation[:self.number_of_panels]), dtype=int)\n",
    "        replaced_panels[indices] = 1\n",
    "\n",
    "        instaltion = (self.observation[:self.number_of_panels] > 0).astype(int)\n",
    "        self.pv_costs -= instaltion.sum() * current_operational_costs\n",
    "\n",
    "        actions_step = np.array(replaced_panels)\n",
    "\n",
    "            \n",
    "        if action > 0:\n",
    "            step_CAPEX, panel_cost_and_inverter = self.calculate_total_CAPEX(actions_step, self.panel_cost_and_inverter)\n",
    "            self.pv_costs -= step_CAPEX\n",
    "            \n",
    "        else:\n",
    "            panel_cost_and_inverter = 0\n",
    "                \n",
    "        next_observation = self._get_obs()\n",
    "\n",
    "        # Calculate the Reslae value\n",
    "        resale = self.calculate_resale(panel_cost_and_inverter, indices) #  ***\n",
    "        \n",
    "        self.pv_costs += resale\n",
    "\n",
    "        \n",
    "        # CALCULATE THE BUDGET INTEREST\n",
    "        current_penalty, due_loans, next_step_penalty = self.calculate_penalty(self.episode_len, self.pv_costs)\n",
    "        \n",
    "        \n",
    "        # CALCULATE THE ENERGY YIELD\n",
    "        exported_revenue, AC_OUTPUT_tot, minimised_revenue = self.calculate_import_export(self.AC_OUTPUT, \n",
    "                                                                          self.elec_df, FiT, self.import_price)        \n",
    "        \n",
    "        pv_costs_observation = - self.pv_costs / 10000\n",
    "        self.observation[self.number_of_panels + 4] = pv_costs_observation\n",
    "        \n",
    "        next_step_penalty_observation = - next_step_penalty / 8000\n",
    "        self.observation[self.number_of_panels + 5] = next_step_penalty_observation\n",
    "        \n",
    "        \n",
    "        # CALCULATE STEP BALANCES\n",
    "        self.fin_balance += self.pv_costs\n",
    "        self.fin_balance += current_penalty\n",
    "        self.fin_balance += float(exported_revenue + minimised_revenue)\n",
    "        \n",
    "        # CALCULATE TOTAL BALANCES\n",
    "        self.fin_balance_tot += self.fin_balance                \n",
    "        \n",
    "        # SUBSTRACT 1 FOR TIMESTEP\n",
    "        self.episode_len -= 1\n",
    "        done = self.episode_len <= 0\n",
    "        \n",
    "        # CALCULATE IRR, NPV AND CARBON INTENSITY\n",
    "        total_cash_flow = self.calculate_irr_and_npv(self.pv_costs, exported_revenue, minimised_revenue, current_penalty)\n",
    "        irr = npf.irr(total_cash_flow) * 100\n",
    "        npv = npf.npv(0.04 ,total_cash_flow)\n",
    "            \n",
    "        # RETURNS AND CALCULATE REWARD\n",
    "        if self.episode_len == 0:\n",
    "            irr_fin = irr\n",
    "            npv_fin = npv\n",
    "        \n",
    "        reward = self.fin_balance / 1000\n",
    "        #reward = self.fin_balance_tot / 1000 if done else 0\n",
    "        \n",
    "        # FAILURE\n",
    "         \n",
    "        survival = self.failure(actions_step)\n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "\n",
    "        for c, p in enumerate(survival):\n",
    "            \n",
    "            if c < self.number_of_panels:\n",
    "\n",
    "                if p - 1 <= abs(self.episode_len - 24):\n",
    "                    self.broke[c] = 1\n",
    "\n",
    "                    self.observation[c] = 0\n",
    "        \n",
    "        # DEGRADATION RATE\n",
    "        # Applying degradation only to panels that are operational (above 0.1 efficiency)\n",
    "        active_panels = self.observation[:self.number_of_panels] > 0.1\n",
    "        degradations = np.random.normal(self.deg_mu, self.deg_std, size=self.number_of_panels) / 100\n",
    "        self.observation[:self.number_of_panels][active_panels] -= degradations[active_panels]\n",
    "        \n",
    "        if not done: \n",
    "        \n",
    "            self.next_cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "            self.next_import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "            self.next_efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "            next_FIT = self.calculate_FiT(self.episode_len, self.next_import_price)\n",
    "        \n",
    "            price_observation = (self.next_import_price - 0.00022499) / (0.0020798 - 0.00022499)\n",
    "            self.observation[self.number_of_panels] = price_observation\n",
    "\n",
    "            FiT_observation = (next_FIT - 0.00022499 * 0.33) / (0.0020798 - 0.00022499 * 0.33)\n",
    "            self.observation[self.number_of_panels + 1] = FiT_observation\n",
    "\n",
    "            eff_observation = (self.next_efficency_develop - 0.999) / (1.156 - 0.999)\n",
    "            self.observation[self.number_of_panels + 2] = eff_observation\n",
    "\n",
    "            cost_per_Wp_observation = (self.next_cost_per_Wp - 0.254) / (1.12 - 0.254) \n",
    "            self.observation[self.number_of_panels + 3] = cost_per_Wp_observation\n",
    "        \n",
    "        \n",
    "        info = {\"step financial balance (eur):\": self.fin_balance,\n",
    "               \"total financial balance: (eur)\": self.fin_balance_tot,\n",
    "               \"internal rate of return\": irr_fin,\n",
    "               \"current_interest\": current_penalty,\n",
    "                \"net present value\": npv_fin}\n",
    "         \n",
    "        \n",
    "        self.previous_observation = self.observation.copy()\n",
    "        \n",
    "        return self.observation, reward, done, False, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c69028",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TrainEnvironment(AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_price_train_arr, Eff_train_arr, CAPEX_JA_train_arr)\n",
    "env_test = TestEnvironment(AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_price_test_arr, Eff_test_arr, CAPEX_JA_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f083fc0",
   "metadata": {
    "code_folding": [
     1
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubaw\\AppData\\Local\\Temp\\ipykernel_12144\\3868030579.py:178: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  value = annual_expense / self.current_budget_constraint\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "class RewardNormalizer:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.mean = 0\n",
    "        self.M2 = 0\n",
    "\n",
    "    def update(self, reward):\n",
    "        self.n += 1\n",
    "        delta = reward - self.mean\n",
    "        self.mean += delta / self.n\n",
    "        delta2 = reward - self.mean\n",
    "        self.M2 += delta * delta2\n",
    "\n",
    "    def normalize(self, reward):\n",
    "        if self.n < 2:\n",
    "            return reward  # Not enough data to normalize\n",
    "        variance = self.M2 / (self.n - 1)\n",
    "        std_dev = variance ** 0.5\n",
    "        return (reward - self.mean) / std_dev \n",
    "\n",
    "    def sample_and_normalize_rewards(self, episodes, environment):\n",
    "        normalized_rewards = []\n",
    "        rewards = []\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            obs = environment.reset()\n",
    "            step = 0\n",
    "            \n",
    "            while not done:\n",
    "                step += 1\n",
    "                random_action = environment.action_space.sample()\n",
    "                obs, reward, done, _, info = environment.step(random_action)\n",
    "                \n",
    "                values = list(info.values())\n",
    "\n",
    "                reward = values[0]\n",
    "\n",
    "                # Assuming the reward to normalize is directly obtained from the environment step\n",
    "                self.update(reward)\n",
    "                normalized_reward = self.normalize(reward)\n",
    "                normalized_rewards.append(normalized_reward)\n",
    "                rewards.append(reward)\n",
    "        \n",
    "        # Calculate the final standard deviation\n",
    "        final_std_dev = (self.M2 / (self.n - 1)) ** 0.5 if self.n > 1 else 0\n",
    "\n",
    "        return normalized_rewards, rewards, self.mean, final_std_dev\n",
    "\n",
    "normalizer = RewardNormalizer()\n",
    "normalized_rewards, rewards, final_mean, final_std_dev = normalizer.sample_and_normalize_rewards(1000, env)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60c5c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4903b5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test3(1, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1279284b",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test1(1000, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e45550",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def make_env(rank: int, seed: int = 0) -> Callable:\n",
    "    def _init() -> gym.Env:\n",
    "        random.seed(seed + rank)\n",
    "        np.random.seed(seed + rank) \n",
    "        env = TrainEnvironment(AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_price_train_arr, Eff_train_arr, CAPEX_JA_train_arr)\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "\n",
    "    return _init\n",
    "# Number of environments to run in parallel\n",
    "num_cpu = 16\n",
    "env = SubprocVecEnv([make_env(i) for i in range(num_cpu)])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4053c411",
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def logarithmic_schedule(initial_value, final_value=0.00001):\n",
    "    \"\"\"\n",
    "    Returns a function that computes a logarithmically decreasing value from initial_value to final_value.\n",
    "    \"\"\"\n",
    "    def func(progress_remaining):\n",
    "        # Avoid taking log of zero by setting a lower limit close to zero\n",
    "        epsilon = 0.0001\n",
    "        progress = max(epsilon, 1 - progress_remaining)\n",
    "        # Calculate the decay factor using a logarithmic scale\n",
    "        return final_value + (initial_value - final_value) * math.log(1/progress)\n",
    "    return func\n",
    "\n",
    "\n",
    "learning_rate = logarithmic_schedule(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36af5784",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "log_path = \"./logs/\"\n",
    "eval_callback = EvalCallback(env_test, best_model_save_path = \"C:/Users/kubaw/Desktop/DELFT/THESIS/CODE/TEST_MODELS/ja24_low/\",\n",
    "                             log_path = log_path, n_eval_episodes = 750, eval_freq=10000,\n",
    "                             deterministic=True, render=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41010d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=dict(pi=[1024, 1024], vf=[1024, 1024]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3408195a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def linear_schedule(initial_value, final_value=0.00001):\n",
    "    \"\"\"\n",
    "    Returns a function that computes a linearly decreasing value from initial_value to final_value.\n",
    "    \"\"\"\n",
    "    def func(progress_remaining):\n",
    "        # Calculate the decrease based on the remaining progress\n",
    "        return final_value + (initial_value - final_value) * progress_remaining\n",
    "    return func\n",
    "\n",
    "# Define the learning rate using the linear schedule\n",
    "learning_rate = linear_schedule(0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "678979b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to C:/Users/kubaw/Desktop/DELFT/THESIS\\CODE/TEST_MODELS/LOGS/logs\\PPO_387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubaw\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x00000167E1791B70> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x00000167E17933D0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 3921  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 8     |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1189        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016887046 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.2        |\n",
      "|    explained_variance   | -0.000337   |\n",
      "|    learning_rate        | 0.000299    |\n",
      "|    loss                 | 27.6        |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 71          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 964         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017102802 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.16       |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.000298    |\n",
      "|    loss                 | 32.3        |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    value_loss           | 71.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 802         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 163         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018324794 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.08       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.000297    |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    value_loss           | 70.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubaw\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kubaw\\AppData\\Local\\Temp\\ipykernel_3616\\3881792178.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  value = annual_expense / self.current_budget_constraint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=160000, episode_reward=27.08 +/- 13.59\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 27.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017697046 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.99       |\n",
      "|    explained_variance   | 0.607       |\n",
      "|    learning_rate        | 0.000296    |\n",
      "|    loss                 | 36.5        |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 67.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 675    |\n",
      "|    iterations      | 5      |\n",
      "|    time_elapsed    | 242    |\n",
      "|    total_timesteps | 163840 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 662         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 296         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016355462 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.000295    |\n",
      "|    loss                 | 34.7        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0334     |\n",
      "|    value_loss           | 64          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 665         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 344         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017402146 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.000294    |\n",
      "|    loss                 | 37.3        |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 66.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 669        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 391        |\n",
      "|    total_timesteps      | 262144     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01638835 |\n",
      "|    clip_fraction        | 0.285      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.65      |\n",
      "|    explained_variance   | 0.43       |\n",
      "|    learning_rate        | 0.000293   |\n",
      "|    loss                 | 28.2       |\n",
      "|    n_updates            | 168        |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    value_loss           | 63.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 672         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 438         |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017827239 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.000292    |\n",
      "|    loss                 | 31.5        |\n",
      "|    n_updates            | 192         |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 59.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=29.49 +/- 11.14\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 29.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014443753 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.41       |\n",
      "|    explained_variance   | 0.382       |\n",
      "|    learning_rate        | 0.000291    |\n",
      "|    loss                 | 28.5        |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 60.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 651    |\n",
      "|    iterations      | 10     |\n",
      "|    time_elapsed    | 502    |\n",
      "|    total_timesteps | 327680 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 553         |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013814912 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.25       |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.00029     |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 55.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 604         |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013437467 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.13       |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.00029     |\n",
      "|    loss                 | 26.8        |\n",
      "|    n_updates            | 264         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 655         |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010376338 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.02       |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 0.000289    |\n",
      "|    loss                 | 29.2        |\n",
      "|    n_updates            | 288         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 56.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 715         |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013487228 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.000288    |\n",
      "|    loss                 | 23.5        |\n",
      "|    n_updates            | 312         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 51.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=480000, episode_reward=28.60 +/- 11.32\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 28.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015452577 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.000287    |\n",
      "|    loss                 | 23.2        |\n",
      "|    n_updates            | 336         |\n",
      "|    policy_gradient_loss | -0.00935    |\n",
      "|    value_loss           | 50.6        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 619    |\n",
      "|    iterations      | 15     |\n",
      "|    time_elapsed    | 794    |\n",
      "|    total_timesteps | 491520 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 617         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 849         |\n",
      "|    total_timesteps      | 524288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009749054 |\n",
      "|    clip_fraction        | 0.0907      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 0.541       |\n",
      "|    learning_rate        | 0.000286    |\n",
      "|    loss                 | 23.7        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00743    |\n",
      "|    value_loss           | 47.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 613          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 908          |\n",
      "|    total_timesteps      | 557056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088140955 |\n",
      "|    clip_fraction        | 0.0679       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 0.58         |\n",
      "|    learning_rate        | 0.000285     |\n",
      "|    loss                 | 24.9         |\n",
      "|    n_updates            | 384          |\n",
      "|    policy_gradient_loss | -0.00635     |\n",
      "|    value_loss           | 48.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 964         |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006121167 |\n",
      "|    clip_fraction        | 0.0609      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.602       |\n",
      "|    learning_rate        | 0.000284    |\n",
      "|    loss                 | 24.8        |\n",
      "|    n_updates            | 408         |\n",
      "|    policy_gradient_loss | -0.00538    |\n",
      "|    value_loss           | 46.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 614          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 1013         |\n",
      "|    total_timesteps      | 622592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066305595 |\n",
      "|    clip_fraction        | 0.0674       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.56        |\n",
      "|    explained_variance   | 0.616        |\n",
      "|    learning_rate        | 0.000283     |\n",
      "|    loss                 | 21.2         |\n",
      "|    n_updates            | 432          |\n",
      "|    policy_gradient_loss | -0.0041      |\n",
      "|    value_loss           | 45           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=31.79 +/- 12.82\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 31.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 640000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057508815 |\n",
      "|    clip_fraction        | 0.0624       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.615        |\n",
      "|    learning_rate        | 0.000282     |\n",
      "|    loss                 | 22.4         |\n",
      "|    n_updates            | 456          |\n",
      "|    policy_gradient_loss | -0.00428     |\n",
      "|    value_loss           | 47.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 608    |\n",
      "|    iterations      | 20     |\n",
      "|    time_elapsed    | 1076   |\n",
      "|    total_timesteps | 655360 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 614         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 1119        |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006315012 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.000281    |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00395    |\n",
      "|    value_loss           | 47.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 619          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 1163         |\n",
      "|    total_timesteps      | 720896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077824574 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.613        |\n",
      "|    learning_rate        | 0.00028      |\n",
      "|    loss                 | 26.5         |\n",
      "|    n_updates            | 504          |\n",
      "|    policy_gradient_loss | -0.00417     |\n",
      "|    value_loss           | 50.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 622          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 1210         |\n",
      "|    total_timesteps      | 753664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057234345 |\n",
      "|    clip_fraction        | 0.0829       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.636        |\n",
      "|    learning_rate        | 0.000279     |\n",
      "|    loss                 | 23.4         |\n",
      "|    n_updates            | 528          |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    value_loss           | 45           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1256        |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008123383 |\n",
      "|    clip_fraction        | 0.061       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.622       |\n",
      "|    learning_rate        | 0.000278    |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 552         |\n",
      "|    policy_gradient_loss | -0.00472    |\n",
      "|    value_loss           | 45.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=800000, episode_reward=34.49 +/- 13.12\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 34.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 800000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054384964 |\n",
      "|    clip_fraction        | 0.0744       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.626        |\n",
      "|    learning_rate        | 0.000277     |\n",
      "|    loss                 | 26.1         |\n",
      "|    n_updates            | 576          |\n",
      "|    policy_gradient_loss | -0.00316     |\n",
      "|    value_loss           | 47.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 619    |\n",
      "|    iterations      | 25     |\n",
      "|    time_elapsed    | 1322   |\n",
      "|    total_timesteps | 819200 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 622          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 1367         |\n",
      "|    total_timesteps      | 851968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050972053 |\n",
      "|    clip_fraction        | 0.0844       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.621        |\n",
      "|    learning_rate        | 0.000276     |\n",
      "|    loss                 | 22.2         |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00253     |\n",
      "|    value_loss           | 46           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 627          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 1410         |\n",
      "|    total_timesteps      | 884736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060210326 |\n",
      "|    clip_fraction        | 0.0547       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.636        |\n",
      "|    learning_rate        | 0.000275     |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 624          |\n",
      "|    policy_gradient_loss | -0.00442     |\n",
      "|    value_loss           | 43.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 631          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 1453         |\n",
      "|    total_timesteps      | 917504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068431823 |\n",
      "|    clip_fraction        | 0.0617       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.646        |\n",
      "|    learning_rate        | 0.000274     |\n",
      "|    loss                 | 23.3         |\n",
      "|    n_updates            | 648          |\n",
      "|    policy_gradient_loss | -0.00399     |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 635          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 1495         |\n",
      "|    total_timesteps      | 950272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061870776 |\n",
      "|    clip_fraction        | 0.0672       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0.645        |\n",
      "|    learning_rate        | 0.000273     |\n",
      "|    loss                 | 23.3         |\n",
      "|    n_updates            | 672          |\n",
      "|    policy_gradient_loss | -0.00412     |\n",
      "|    value_loss           | 44.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=35.26 +/- 12.12\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005680425 |\n",
      "|    clip_fraction        | 0.0622      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.000272    |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 696         |\n",
      "|    policy_gradient_loss | -0.00314    |\n",
      "|    value_loss           | 42          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 631    |\n",
      "|    iterations      | 30     |\n",
      "|    time_elapsed    | 1557   |\n",
      "|    total_timesteps | 983040 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 634         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 1599        |\n",
      "|    total_timesteps      | 1015808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005958656 |\n",
      "|    clip_fraction        | 0.0595      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.000271    |\n",
      "|    loss                 | 25.2        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00416    |\n",
      "|    value_loss           | 44.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 1643        |\n",
      "|    total_timesteps      | 1048576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005210744 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.000271    |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 744         |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    value_loss           | 46.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 641          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 1685         |\n",
      "|    total_timesteps      | 1081344      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068518445 |\n",
      "|    clip_fraction        | 0.0651       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.64         |\n",
      "|    learning_rate        | 0.00027      |\n",
      "|    loss                 | 25           |\n",
      "|    n_updates            | 768          |\n",
      "|    policy_gradient_loss | -0.00359     |\n",
      "|    value_loss           | 47.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 645          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 1727         |\n",
      "|    total_timesteps      | 1114112      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065231416 |\n",
      "|    clip_fraction        | 0.0685       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0.652        |\n",
      "|    learning_rate        | 0.000269     |\n",
      "|    loss                 | 22.7         |\n",
      "|    n_updates            | 792          |\n",
      "|    policy_gradient_loss | -0.00423     |\n",
      "|    value_loss           | 46.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1120000, episode_reward=34.67 +/- 12.57\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 34.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1120000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066793975 |\n",
      "|    clip_fraction        | 0.0715       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.655        |\n",
      "|    learning_rate        | 0.000268     |\n",
      "|    loss                 | 21.9         |\n",
      "|    n_updates            | 816          |\n",
      "|    policy_gradient_loss | -0.00356     |\n",
      "|    value_loss           | 48           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 640     |\n",
      "|    iterations      | 35      |\n",
      "|    time_elapsed    | 1790    |\n",
      "|    total_timesteps | 1146880 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 643          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 1833         |\n",
      "|    total_timesteps      | 1179648      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061630323 |\n",
      "|    clip_fraction        | 0.0732       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.665        |\n",
      "|    learning_rate        | 0.000267     |\n",
      "|    loss                 | 22.6         |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00289     |\n",
      "|    value_loss           | 45.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 646         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 1875        |\n",
      "|    total_timesteps      | 1212416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009010244 |\n",
      "|    clip_fraction        | 0.0757      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.000266    |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 864         |\n",
      "|    policy_gradient_loss | -0.00449    |\n",
      "|    value_loss           | 44.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 649          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 1918         |\n",
      "|    total_timesteps      | 1245184      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075412076 |\n",
      "|    clip_fraction        | 0.0605       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 0.645        |\n",
      "|    learning_rate        | 0.000265     |\n",
      "|    loss                 | 23.8         |\n",
      "|    n_updates            | 888          |\n",
      "|    policy_gradient_loss | -0.00347     |\n",
      "|    value_loss           | 46.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 1959         |\n",
      "|    total_timesteps      | 1277952      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062362296 |\n",
      "|    clip_fraction        | 0.0776       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 0.653        |\n",
      "|    learning_rate        | 0.000264     |\n",
      "|    loss                 | 20.2         |\n",
      "|    n_updates            | 912          |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 42.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=35.03 +/- 12.77\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004838506 |\n",
      "|    clip_fraction        | 0.0696      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.000263    |\n",
      "|    loss                 | 23          |\n",
      "|    n_updates            | 936         |\n",
      "|    policy_gradient_loss | -0.00268    |\n",
      "|    value_loss           | 44.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 634     |\n",
      "|    iterations      | 40      |\n",
      "|    time_elapsed    | 2065    |\n",
      "|    total_timesteps | 1310720 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 630          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 2130         |\n",
      "|    total_timesteps      | 1343488      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059271213 |\n",
      "|    clip_fraction        | 0.0577       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.659        |\n",
      "|    learning_rate        | 0.000262     |\n",
      "|    loss                 | 20.7         |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0036      |\n",
      "|    value_loss           | 42.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 627          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 2192         |\n",
      "|    total_timesteps      | 1376256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058651688 |\n",
      "|    clip_fraction        | 0.0669       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0.666        |\n",
      "|    learning_rate        | 0.000261     |\n",
      "|    loss                 | 20.9         |\n",
      "|    n_updates            | 984          |\n",
      "|    policy_gradient_loss | -0.00291     |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 624         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 2254        |\n",
      "|    total_timesteps      | 1409024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006852123 |\n",
      "|    clip_fraction        | 0.0583      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.00026     |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 1008        |\n",
      "|    policy_gradient_loss | -0.00296    |\n",
      "|    value_loss           | 41.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=35.35 +/- 12.99\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007768878 |\n",
      "|    clip_fraction        | 0.0698      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.000259    |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 1032        |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 43.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 613     |\n",
      "|    iterations      | 44      |\n",
      "|    time_elapsed    | 2348    |\n",
      "|    total_timesteps | 1441792 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 2411        |\n",
      "|    total_timesteps      | 1474560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004517084 |\n",
      "|    clip_fraction        | 0.0543      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.000258    |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 1056        |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    value_loss           | 45.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 2475         |\n",
      "|    total_timesteps      | 1507328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049355943 |\n",
      "|    clip_fraction        | 0.0684       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.669        |\n",
      "|    learning_rate        | 0.000257     |\n",
      "|    loss                 | 21.2         |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    value_loss           | 44.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 2537        |\n",
      "|    total_timesteps      | 1540096     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006058248 |\n",
      "|    clip_fraction        | 0.0653      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.000256    |\n",
      "|    loss                 | 20.6        |\n",
      "|    n_updates            | 1104        |\n",
      "|    policy_gradient_loss | -0.00302    |\n",
      "|    value_loss           | 45.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 2599         |\n",
      "|    total_timesteps      | 1572864      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055959094 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.683        |\n",
      "|    learning_rate        | 0.000255     |\n",
      "|    loss                 | 22.9         |\n",
      "|    n_updates            | 1128         |\n",
      "|    policy_gradient_loss | -0.00249     |\n",
      "|    value_loss           | 44.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=35.33 +/- 12.83\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018583365 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.000254    |\n",
      "|    loss                 | 23.8        |\n",
      "|    n_updates            | 1152        |\n",
      "|    policy_gradient_loss | -0.00298    |\n",
      "|    value_loss           | 46          |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 595     |\n",
      "|    iterations      | 49      |\n",
      "|    time_elapsed    | 2697    |\n",
      "|    total_timesteps | 1605632 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 592          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 2764         |\n",
      "|    total_timesteps      | 1638400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061879624 |\n",
      "|    clip_fraction        | 0.06         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.67         |\n",
      "|    learning_rate        | 0.000253     |\n",
      "|    loss                 | 20.3         |\n",
      "|    n_updates            | 1176         |\n",
      "|    policy_gradient_loss | -0.00281     |\n",
      "|    value_loss           | 45.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 590         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 2829        |\n",
      "|    total_timesteps      | 1671168     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006634279 |\n",
      "|    clip_fraction        | 0.053       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.000252    |\n",
      "|    loss                 | 21.8        |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.00301    |\n",
      "|    value_loss           | 45.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 588         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 2895        |\n",
      "|    total_timesteps      | 1703936     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006809706 |\n",
      "|    clip_fraction        | 0.0599      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.000252    |\n",
      "|    loss                 | 20.5        |\n",
      "|    n_updates            | 1224        |\n",
      "|    policy_gradient_loss | -0.00297    |\n",
      "|    value_loss           | 43.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 586          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 2961         |\n",
      "|    total_timesteps      | 1736704      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051318146 |\n",
      "|    clip_fraction        | 0.0637       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.679        |\n",
      "|    learning_rate        | 0.000251     |\n",
      "|    loss                 | 18.9         |\n",
      "|    n_updates            | 1248         |\n",
      "|    policy_gradient_loss | -0.00312     |\n",
      "|    value_loss           | 40.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=34.51 +/- 12.14\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 34.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1760000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052563483 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.674        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 21.3         |\n",
      "|    n_updates            | 1272         |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    value_loss           | 43           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 578     |\n",
      "|    iterations      | 54      |\n",
      "|    time_elapsed    | 3061    |\n",
      "|    total_timesteps | 1769472 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 576          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 3127         |\n",
      "|    total_timesteps      | 1802240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055800956 |\n",
      "|    clip_fraction        | 0.0696       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.685        |\n",
      "|    learning_rate        | 0.000249     |\n",
      "|    loss                 | 19.1         |\n",
      "|    n_updates            | 1296         |\n",
      "|    policy_gradient_loss | -0.00247     |\n",
      "|    value_loss           | 42.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 574          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 3191         |\n",
      "|    total_timesteps      | 1835008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049780365 |\n",
      "|    clip_fraction        | 0.0609       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.000248     |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    value_loss           | 42.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 573         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 3256        |\n",
      "|    total_timesteps      | 1867776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004672358 |\n",
      "|    clip_fraction        | 0.0526      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.000247    |\n",
      "|    loss                 | 21.9        |\n",
      "|    n_updates            | 1344        |\n",
      "|    policy_gradient_loss | -0.00318    |\n",
      "|    value_loss           | 40.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 572          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 3321         |\n",
      "|    total_timesteps      | 1900544      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055733714 |\n",
      "|    clip_fraction        | 0.0562       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.968       |\n",
      "|    explained_variance   | 0.683        |\n",
      "|    learning_rate        | 0.000246     |\n",
      "|    loss                 | 19.6         |\n",
      "|    n_updates            | 1368         |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    value_loss           | 39           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=35.15 +/- 13.12\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 35.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1920000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047842073 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.966       |\n",
      "|    explained_variance   | 0.681        |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | 22.8         |\n",
      "|    n_updates            | 1392         |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 43.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 565     |\n",
      "|    iterations      | 59      |\n",
      "|    time_elapsed    | 3420    |\n",
      "|    total_timesteps | 1933312 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 564        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 3485       |\n",
      "|    total_timesteps      | 1966080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00657424 |\n",
      "|    clip_fraction        | 0.0746     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.957     |\n",
      "|    explained_variance   | 0.694      |\n",
      "|    learning_rate        | 0.000244   |\n",
      "|    loss                 | 20.3       |\n",
      "|    n_updates            | 1416       |\n",
      "|    policy_gradient_loss | -0.000979  |\n",
      "|    value_loss           | 41.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 562         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 3550        |\n",
      "|    total_timesteps      | 1998848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007588461 |\n",
      "|    clip_fraction        | 0.0519      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.000243    |\n",
      "|    loss                 | 19          |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.00314    |\n",
      "|    value_loss           | 41.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 562          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 3613         |\n",
      "|    total_timesteps      | 2031616      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057988837 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.944       |\n",
      "|    explained_variance   | 0.684        |\n",
      "|    learning_rate        | 0.000242     |\n",
      "|    loss                 | 20.3         |\n",
      "|    n_updates            | 1464         |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    value_loss           | 43.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 561          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 3674         |\n",
      "|    total_timesteps      | 2064384      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055770907 |\n",
      "|    clip_fraction        | 0.0615       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.925       |\n",
      "|    explained_variance   | 0.682        |\n",
      "|    learning_rate        | 0.000241     |\n",
      "|    loss                 | 20.2         |\n",
      "|    n_updates            | 1488         |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    value_loss           | 43.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=34.49 +/- 12.72\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 34.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2080000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005003686 |\n",
      "|    clip_fraction        | 0.0612      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.917      |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.00024     |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 1512        |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    value_loss           | 41.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 556     |\n",
      "|    iterations      | 64      |\n",
      "|    time_elapsed    | 3768    |\n",
      "|    total_timesteps | 2097152 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 556         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 3829        |\n",
      "|    total_timesteps      | 2129920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003680475 |\n",
      "|    clip_fraction        | 0.0535      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.923      |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.000239    |\n",
      "|    loss                 | 20.1        |\n",
      "|    n_updates            | 1536        |\n",
      "|    policy_gradient_loss | -0.00261    |\n",
      "|    value_loss           | 39.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 555          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 3890         |\n",
      "|    total_timesteps      | 2162688      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040362114 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.926       |\n",
      "|    explained_variance   | 0.691        |\n",
      "|    learning_rate        | 0.000238     |\n",
      "|    loss                 | 18.8         |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 39.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 555          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 3950         |\n",
      "|    total_timesteps      | 2195456      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045453343 |\n",
      "|    clip_fraction        | 0.0696       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.953       |\n",
      "|    explained_variance   | 0.676        |\n",
      "|    learning_rate        | 0.000237     |\n",
      "|    loss                 | 22           |\n",
      "|    n_updates            | 1584         |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 555         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 4011        |\n",
      "|    total_timesteps      | 2228224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005406465 |\n",
      "|    clip_fraction        | 0.0513      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.941      |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.000236    |\n",
      "|    loss                 | 24.3        |\n",
      "|    n_updates            | 1608        |\n",
      "|    policy_gradient_loss | -0.00252    |\n",
      "|    value_loss           | 46.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=34.67 +/- 12.86\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 34.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2240000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004821037 |\n",
      "|    clip_fraction        | 0.0528      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.984      |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.000235    |\n",
      "|    loss                 | 19.3        |\n",
      "|    n_updates            | 1632        |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    value_loss           | 44.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 550     |\n",
      "|    iterations      | 69      |\n",
      "|    time_elapsed    | 4104    |\n",
      "|    total_timesteps | 2260992 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 550         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 4164        |\n",
      "|    total_timesteps      | 2293760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006450448 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.98       |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.000234    |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 1656        |\n",
      "|    policy_gradient_loss | -0.00264    |\n",
      "|    value_loss           | 39.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 550        |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 4224       |\n",
      "|    total_timesteps      | 2326528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00727342 |\n",
      "|    clip_fraction        | 0.0532     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.974     |\n",
      "|    explained_variance   | 0.674      |\n",
      "|    learning_rate        | 0.000233   |\n",
      "|    loss                 | 18.2       |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0022    |\n",
      "|    value_loss           | 41.1       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 550          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 4285         |\n",
      "|    total_timesteps      | 2359296      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059439335 |\n",
      "|    clip_fraction        | 0.0614       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.962       |\n",
      "|    explained_variance   | 0.669        |\n",
      "|    learning_rate        | 0.000233     |\n",
      "|    loss                 | 23.4         |\n",
      "|    n_updates            | 1704         |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    value_loss           | 44.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 550          |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 4347         |\n",
      "|    total_timesteps      | 2392064      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050271032 |\n",
      "|    clip_fraction        | 0.0498       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.897       |\n",
      "|    explained_variance   | 0.684        |\n",
      "|    learning_rate        | 0.000232     |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 1728         |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    value_loss           | 41.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=35.35 +/- 12.71\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 35.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2400000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053279833 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.913       |\n",
      "|    explained_variance   | 0.668        |\n",
      "|    learning_rate        | 0.000231     |\n",
      "|    loss                 | 20.9         |\n",
      "|    n_updates            | 1752         |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 42.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 546     |\n",
      "|    iterations      | 74      |\n",
      "|    time_elapsed    | 4439    |\n",
      "|    total_timesteps | 2424832 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 546          |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 4500         |\n",
      "|    total_timesteps      | 2457600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064306986 |\n",
      "|    clip_fraction        | 0.0501       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.917       |\n",
      "|    explained_variance   | 0.676        |\n",
      "|    learning_rate        | 0.00023      |\n",
      "|    loss                 | 21.5         |\n",
      "|    n_updates            | 1776         |\n",
      "|    policy_gradient_loss | -0.00241     |\n",
      "|    value_loss           | 43.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 545         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 4562        |\n",
      "|    total_timesteps      | 2490368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004725259 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.901      |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.000229    |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    value_loss           | 43.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 4621         |\n",
      "|    total_timesteps      | 2523136      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063987286 |\n",
      "|    clip_fraction        | 0.0606       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.895       |\n",
      "|    explained_variance   | 0.677        |\n",
      "|    learning_rate        | 0.000228     |\n",
      "|    loss                 | 17.7         |\n",
      "|    n_updates            | 1824         |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    value_loss           | 42.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 546         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 4680        |\n",
      "|    total_timesteps      | 2555904     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005113746 |\n",
      "|    clip_fraction        | 0.0613      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.000227    |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 1848        |\n",
      "|    policy_gradient_loss | -0.00163    |\n",
      "|    value_loss           | 44.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=35.43 +/- 13.54\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2560000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006497221 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.886      |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.000226    |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 1872        |\n",
      "|    policy_gradient_loss | -0.0025     |\n",
      "|    value_loss           | 38.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 542     |\n",
      "|    iterations      | 79      |\n",
      "|    time_elapsed    | 4771    |\n",
      "|    total_timesteps | 2588672 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 542        |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 4831       |\n",
      "|    total_timesteps      | 2621440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00507019 |\n",
      "|    clip_fraction        | 0.0555     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.904     |\n",
      "|    explained_variance   | 0.687      |\n",
      "|    learning_rate        | 0.000225   |\n",
      "|    loss                 | 19.4       |\n",
      "|    n_updates            | 1896       |\n",
      "|    policy_gradient_loss | -0.00253   |\n",
      "|    value_loss           | 42.3       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 542        |\n",
      "|    iterations           | 81         |\n",
      "|    time_elapsed         | 4890       |\n",
      "|    total_timesteps      | 2654208    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00468238 |\n",
      "|    clip_fraction        | 0.0577     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.906     |\n",
      "|    explained_variance   | 0.671      |\n",
      "|    learning_rate        | 0.000224   |\n",
      "|    loss                 | 22.9       |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.00269   |\n",
      "|    value_loss           | 42.3       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 4950         |\n",
      "|    total_timesteps      | 2686976      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040484807 |\n",
      "|    clip_fraction        | 0.0578       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.898       |\n",
      "|    explained_variance   | 0.672        |\n",
      "|    learning_rate        | 0.000223     |\n",
      "|    loss                 | 22.7         |\n",
      "|    n_updates            | 1944         |\n",
      "|    policy_gradient_loss | -0.0024      |\n",
      "|    value_loss           | 42.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 5009        |\n",
      "|    total_timesteps      | 2719744     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007855305 |\n",
      "|    clip_fraction        | 0.0592      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.912      |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.000222    |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 1968        |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    value_loss           | 43.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=35.18 +/- 13.15\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 35.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2720000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048048487 |\n",
      "|    clip_fraction        | 0.0546       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.951       |\n",
      "|    explained_variance   | 0.659        |\n",
      "|    learning_rate        | 0.000221     |\n",
      "|    loss                 | 21.1         |\n",
      "|    n_updates            | 1992         |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    value_loss           | 44.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 539     |\n",
      "|    iterations      | 84      |\n",
      "|    time_elapsed    | 5099    |\n",
      "|    total_timesteps | 2752512 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 539         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 5158        |\n",
      "|    total_timesteps      | 2785280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006319752 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.932      |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.00022     |\n",
      "|    loss                 | 20.6        |\n",
      "|    n_updates            | 2016        |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    value_loss           | 44.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 540          |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 5217         |\n",
      "|    total_timesteps      | 2818048      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067351796 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.912       |\n",
      "|    explained_variance   | 0.66         |\n",
      "|    learning_rate        | 0.000219     |\n",
      "|    loss                 | 21.4         |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 44.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 540         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 5274        |\n",
      "|    total_timesteps      | 2850816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004491441 |\n",
      "|    clip_fraction        | 0.0648      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.892      |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.000218    |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 2064        |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    value_loss           | 44.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2880000, episode_reward=35.59 +/- 12.91\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2880000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006303044 |\n",
      "|    clip_fraction        | 0.0533      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.855      |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.000217    |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 2088        |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 44.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 537     |\n",
      "|    iterations      | 88      |\n",
      "|    time_elapsed    | 5364    |\n",
      "|    total_timesteps | 2883584 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 537         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 5423        |\n",
      "|    total_timesteps      | 2916352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006595499 |\n",
      "|    clip_fraction        | 0.0568      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.789      |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.000216    |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 2112        |\n",
      "|    policy_gradient_loss | -0.00229    |\n",
      "|    value_loss           | 44.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 5481         |\n",
      "|    total_timesteps      | 2949120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065518944 |\n",
      "|    clip_fraction        | 0.0578       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.733       |\n",
      "|    explained_variance   | 0.682        |\n",
      "|    learning_rate        | 0.000215     |\n",
      "|    loss                 | 22.5         |\n",
      "|    n_updates            | 2136         |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 5540         |\n",
      "|    total_timesteps      | 2981888      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038122335 |\n",
      "|    clip_fraction        | 0.0551       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.744       |\n",
      "|    explained_variance   | 0.683        |\n",
      "|    learning_rate        | 0.000214     |\n",
      "|    loss                 | 20.5         |\n",
      "|    n_updates            | 2160         |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 43.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 5596         |\n",
      "|    total_timesteps      | 3014656      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044019017 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.729       |\n",
      "|    explained_variance   | 0.706        |\n",
      "|    learning_rate        | 0.000214     |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 2184         |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 38.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3040000, episode_reward=34.01 +/- 12.78\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 34          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003953959 |\n",
      "|    clip_fraction        | 0.0493      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.722      |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.000213    |\n",
      "|    loss                 | 18.6        |\n",
      "|    n_updates            | 2208        |\n",
      "|    policy_gradient_loss | -0.00228    |\n",
      "|    value_loss           | 37          |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 536     |\n",
      "|    iterations      | 93      |\n",
      "|    time_elapsed    | 5683    |\n",
      "|    total_timesteps | 3047424 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 536        |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 5739       |\n",
      "|    total_timesteps      | 3080192    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00444441 |\n",
      "|    clip_fraction        | 0.0465     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.733     |\n",
      "|    explained_variance   | 0.693      |\n",
      "|    learning_rate        | 0.000212   |\n",
      "|    loss                 | 18.6       |\n",
      "|    n_updates            | 2232       |\n",
      "|    policy_gradient_loss | -0.00217   |\n",
      "|    value_loss           | 39.9       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 537         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 5796        |\n",
      "|    total_timesteps      | 3112960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004878043 |\n",
      "|    clip_fraction        | 0.0484      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.74       |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.000211    |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 2256        |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    value_loss           | 39.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 537         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 5854        |\n",
      "|    total_timesteps      | 3145728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005810477 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.707      |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.00021     |\n",
      "|    loss                 | 18.1        |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.00258    |\n",
      "|    value_loss           | 40.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 537          |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 5910         |\n",
      "|    total_timesteps      | 3178496      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042867744 |\n",
      "|    clip_fraction        | 0.0541       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.67        |\n",
      "|    explained_variance   | 0.696        |\n",
      "|    learning_rate        | 0.000209     |\n",
      "|    loss                 | 18.8         |\n",
      "|    n_updates            | 2304         |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    value_loss           | 39.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3200000, episode_reward=34.60 +/- 12.25\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 34.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3200000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035629503 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.67        |\n",
      "|    explained_variance   | 0.704        |\n",
      "|    learning_rate        | 0.000208     |\n",
      "|    loss                 | 18.1         |\n",
      "|    n_updates            | 2328         |\n",
      "|    policy_gradient_loss | -0.00292     |\n",
      "|    value_loss           | 38.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 535     |\n",
      "|    iterations      | 98      |\n",
      "|    time_elapsed    | 5998    |\n",
      "|    total_timesteps | 3211264 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 535         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 6054        |\n",
      "|    total_timesteps      | 3244032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006700354 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.657      |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.000207    |\n",
      "|    loss                 | 18.7        |\n",
      "|    n_updates            | 2352        |\n",
      "|    policy_gradient_loss | -0.00235    |\n",
      "|    value_loss           | 37.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 536         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 6110        |\n",
      "|    total_timesteps      | 3276800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004625962 |\n",
      "|    clip_fraction        | 0.0514      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.661      |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.000206    |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 2376        |\n",
      "|    policy_gradient_loss | -0.00224    |\n",
      "|    value_loss           | 36.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 536         |\n",
      "|    iterations           | 101         |\n",
      "|    time_elapsed         | 6166        |\n",
      "|    total_timesteps      | 3309568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005060712 |\n",
      "|    clip_fraction        | 0.052       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.673      |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.000205    |\n",
      "|    loss                 | 18.2        |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.00274    |\n",
      "|    value_loss           | 38          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 537          |\n",
      "|    iterations           | 102          |\n",
      "|    time_elapsed         | 6223         |\n",
      "|    total_timesteps      | 3342336      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044405246 |\n",
      "|    clip_fraction        | 0.0464       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.667       |\n",
      "|    explained_variance   | 0.702        |\n",
      "|    learning_rate        | 0.000204     |\n",
      "|    loss                 | 19.2         |\n",
      "|    n_updates            | 2424         |\n",
      "|    policy_gradient_loss | -0.00254     |\n",
      "|    value_loss           | 39.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3360000, episode_reward=35.67 +/- 13.35\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 35.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3360000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039913463 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.659       |\n",
      "|    explained_variance   | 0.702        |\n",
      "|    learning_rate        | 0.000203     |\n",
      "|    loss                 | 21.4         |\n",
      "|    n_updates            | 2448         |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    value_loss           | 40.6         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 534     |\n",
      "|    iterations      | 103     |\n",
      "|    time_elapsed    | 6311    |\n",
      "|    total_timesteps | 3375104 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 535          |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 6368         |\n",
      "|    total_timesteps      | 3407872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038336336 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.668       |\n",
      "|    explained_variance   | 0.693        |\n",
      "|    learning_rate        | 0.000202     |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 2472         |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    value_loss           | 42.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 535         |\n",
      "|    iterations           | 105         |\n",
      "|    time_elapsed         | 6424        |\n",
      "|    total_timesteps      | 3440640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004869872 |\n",
      "|    clip_fraction        | 0.0501      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.000201    |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 2496        |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    value_loss           | 40.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 535          |\n",
      "|    iterations           | 106          |\n",
      "|    time_elapsed         | 6482         |\n",
      "|    total_timesteps      | 3473408      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067307157 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.647       |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.0002       |\n",
      "|    loss                 | 19.6         |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    value_loss           | 41.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 536          |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 6540         |\n",
      "|    total_timesteps      | 3506176      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051612416 |\n",
      "|    clip_fraction        | 0.0512       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.684       |\n",
      "|    explained_variance   | 0.69         |\n",
      "|    learning_rate        | 0.000199     |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 2544         |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 39.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3520000, episode_reward=34.85 +/- 13.81\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 34.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3520000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038354218 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.668       |\n",
      "|    explained_variance   | 0.69         |\n",
      "|    learning_rate        | 0.000198     |\n",
      "|    loss                 | 20.9         |\n",
      "|    n_updates            | 2568         |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    value_loss           | 40.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 533     |\n",
      "|    iterations      | 108     |\n",
      "|    time_elapsed    | 6628    |\n",
      "|    total_timesteps | 3538944 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 534          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 6686         |\n",
      "|    total_timesteps      | 3571712      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038604266 |\n",
      "|    clip_fraction        | 0.0519       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.677       |\n",
      "|    explained_variance   | 0.682        |\n",
      "|    learning_rate        | 0.000197     |\n",
      "|    loss                 | 19.6         |\n",
      "|    n_updates            | 2592         |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    value_loss           | 42.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 534         |\n",
      "|    iterations           | 110         |\n",
      "|    time_elapsed         | 6744        |\n",
      "|    total_timesteps      | 3604480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004257274 |\n",
      "|    clip_fraction        | 0.0438      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.683      |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.000196    |\n",
      "|    loss                 | 20.2        |\n",
      "|    n_updates            | 2616        |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 42.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 534          |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 6801         |\n",
      "|    total_timesteps      | 3637248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066052703 |\n",
      "|    clip_fraction        | 0.0491       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.665       |\n",
      "|    explained_variance   | 0.674        |\n",
      "|    learning_rate        | 0.000195     |\n",
      "|    loss                 | 19.8         |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.00178     |\n",
      "|    value_loss           | 42           |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 535        |\n",
      "|    iterations           | 112        |\n",
      "|    time_elapsed         | 6859       |\n",
      "|    total_timesteps      | 3670016    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00442305 |\n",
      "|    clip_fraction        | 0.0438     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.662     |\n",
      "|    explained_variance   | 0.675      |\n",
      "|    learning_rate        | 0.000195   |\n",
      "|    loss                 | 18.5       |\n",
      "|    n_updates            | 2664       |\n",
      "|    policy_gradient_loss | -0.00211   |\n",
      "|    value_loss           | 40.9       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3680000, episode_reward=35.50 +/- 13.85\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 35.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3680000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048039816 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.639       |\n",
      "|    explained_variance   | 0.694        |\n",
      "|    learning_rate        | 0.000194     |\n",
      "|    loss                 | 18.8         |\n",
      "|    n_updates            | 2688         |\n",
      "|    policy_gradient_loss | -0.00236     |\n",
      "|    value_loss           | 38.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 533     |\n",
      "|    iterations      | 113     |\n",
      "|    time_elapsed    | 6946    |\n",
      "|    total_timesteps | 3702784 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 533         |\n",
      "|    iterations           | 114         |\n",
      "|    time_elapsed         | 7001        |\n",
      "|    total_timesteps      | 3735552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004595237 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.000193    |\n",
      "|    loss                 | 21.1        |\n",
      "|    n_updates            | 2712        |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    value_loss           | 46          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 533          |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 7058         |\n",
      "|    total_timesteps      | 3768320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035918856 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.609       |\n",
      "|    explained_variance   | 0.691        |\n",
      "|    learning_rate        | 0.000192     |\n",
      "|    loss                 | 19.4         |\n",
      "|    n_updates            | 2736         |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    value_loss           | 41.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 534          |\n",
      "|    iterations           | 116          |\n",
      "|    time_elapsed         | 7112         |\n",
      "|    total_timesteps      | 3801088      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040644268 |\n",
      "|    clip_fraction        | 0.048        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.592       |\n",
      "|    explained_variance   | 0.686        |\n",
      "|    learning_rate        | 0.000191     |\n",
      "|    loss                 | 20.5         |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 39.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 535         |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 7161        |\n",
      "|    total_timesteps      | 3833856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004201303 |\n",
      "|    clip_fraction        | 0.0438      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.00019     |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 2784        |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    value_loss           | 40.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3840000, episode_reward=35.05 +/- 12.99\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 35.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3840000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005920025 |\n",
      "|    clip_fraction        | 0.0464      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.000189    |\n",
      "|    loss                 | 19          |\n",
      "|    n_updates            | 2808        |\n",
      "|    policy_gradient_loss | -0.00175    |\n",
      "|    value_loss           | 41          |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 535     |\n",
      "|    iterations      | 118     |\n",
      "|    time_elapsed    | 7226    |\n",
      "|    total_timesteps | 3866624 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 535          |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 7275         |\n",
      "|    total_timesteps      | 3899392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034047742 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.559       |\n",
      "|    explained_variance   | 0.696        |\n",
      "|    learning_rate        | 0.000188     |\n",
      "|    loss                 | 19.6         |\n",
      "|    n_updates            | 2832         |\n",
      "|    policy_gradient_loss | -0.00248     |\n",
      "|    value_loss           | 39.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 536          |\n",
      "|    iterations           | 120          |\n",
      "|    time_elapsed         | 7327         |\n",
      "|    total_timesteps      | 3932160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040518213 |\n",
      "|    clip_fraction        | 0.0413       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.567       |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 0.000187     |\n",
      "|    loss                 | 18           |\n",
      "|    n_updates            | 2856         |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    value_loss           | 40.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 537          |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 7370         |\n",
      "|    total_timesteps      | 3964928      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037208232 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.536       |\n",
      "|    explained_variance   | 0.687        |\n",
      "|    learning_rate        | 0.000186     |\n",
      "|    loss                 | 22.9         |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.00241     |\n",
      "|    value_loss           | 40.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 122          |\n",
      "|    time_elapsed         | 7420         |\n",
      "|    total_timesteps      | 3997696      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043428093 |\n",
      "|    clip_fraction        | 0.048        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.562       |\n",
      "|    explained_variance   | 0.695        |\n",
      "|    learning_rate        | 0.000185     |\n",
      "|    loss                 | 21.5         |\n",
      "|    n_updates            | 2904         |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    value_loss           | 41.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, learning_rate \u001b[38;5;241m=\u001b[39m learning_rate, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m, n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m, policy_kwargs \u001b[38;5;241m=\u001b[39m policy_kwargs, gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m,  verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tensorboard_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/kubaw/Desktop/DELFT/THESIS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCODE/TEST_MODELS/LOGS/logs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m TIMESTEPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000000\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:313\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:207\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m approx_kl_divs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Do a complete pass on the rollout buffer\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rollout_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[0;32m    208\u001b[0m     actions \u001b[38;5;241m=\u001b[39m rollout_data\u001b[38;5;241m.\u001b[39mactions\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;66;03m# Convert discrete action from float to long\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:504\u001b[0m, in \u001b[0;36mRolloutBuffer.get\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    502\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs:\n\u001b[1;32m--> 504\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m     start_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:515\u001b[0m, in \u001b[0;36mRolloutBuffer._get_samples\u001b[1;34m(self, batch_inds, env)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_samples\u001b[39m(\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    509\u001b[0m     batch_inds: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m    510\u001b[0m     env: Optional[VecNormalize] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RolloutBufferSamples:\n\u001b[0;32m    512\u001b[0m     data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations[batch_inds],\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[batch_inds],\n\u001b[1;32m--> 515\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvantages[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturns[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RolloutBufferSamples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_torch, data)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, learning_rate = learning_rate, batch_size = 1024, n_epochs = 24, policy_kwargs = policy_kwargs, gamma = 0.99,  verbose=1, tensorboard_log = \"C:/Users/kubaw/Desktop/DELFT/THESIS\\CODE/TEST_MODELS/LOGS/logs\")\n",
    "TIMESTEPS = 10000000\n",
    "model.learn(total_timesteps = TIMESTEPS, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa5c9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r\"C:\\Users\\kubaw\\Desktop\\DELFT\\THESIS\\CODE\\TEST_MODELS\\FINAL24_ja_low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69a9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(r\"C:\\Users\\kubaw\\Desktop\\DELFT\\THESIS\\CODE\\TEST_MODELS\\FINAL24_ja_low.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d69968a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act: 9 \n",
      " Obs: [0.8506809  0.8584513  0.9979332  0.9444468  0.94620717 0.8702701\n",
      " 0.89418256 0.9858364  0.99208087 0.         0.         0.9894404\n",
      " 0.9902745  0.9319078  0.9669751  0.930064   0.9864388  0.9897222\n",
      " 0.8567538  0.99934334 0.9690382  0.9883636  0.98566294 0.96554\n",
      " 0.07780816 0.08083121 0.04069865 0.6843568  0.22262634 0.06385198\n",
      " 0.4304    ] \n",
      " Balance 934.6484733204852\n",
      "Act: 1 \n",
      " Obs: [0.83448714 0.84543145 0.9839316  0.92941517 0.9384754  0.8553898\n",
      " 0.88254833 0.9720743  0.97176784 0.99386346 0.         0.98887855\n",
      " 0.9821276  0.918809   0.9589939  0.9213087  0.971079   0.98773867\n",
      " 0.8422795  0.9796777  0.9611708  0.97831863 0.9790401  0.9443\n",
      " 0.0921257  0.08930569 0.04105865 0.6808497  0.09113253 0.0676831\n",
      " 0.9064    ] \n",
      " Balance 1145.615082726474\n",
      "Act: 3 \n",
      " Obs: [0.98640937 0.8348458  0.97125524 0.91823715 0.92682475 0.8490568\n",
      " 0.8809269  0.96460253 0.964308   0.98867255 0.9894532  0.97657263\n",
      " 0.974911   0.9099361  0.9591434  0.9067026  0.9565509  0.9706006\n",
      " 0.9933011  0.9692458  0.94500977 0.9636699  0.9730638  0.9357928\n",
      " 0.09561976 0.0552629  0.04106111 0.5943884  0.14136715 0.07174408\n",
      " 0.5552    ] \n",
      " Balance 940.3085509209191\n",
      "Act: 2 \n",
      " Obs: [0.97475624 0.9986366  0.9594475  0.9080488  0.9212707  0.9913041\n",
      " 0.86800444 0.9559388  0.9587844  0.97639394 0.96464133 0.97236705\n",
      " 0.9660781  0.897618   0.9488707  0.8924981  0.9341199  0.95408237\n",
      " 0.9879687  0.9632052  0.9203248  0.9510968  0.96292406 0.9245241\n",
      " 0.10472413 0.07794939 0.04107094 0.5966404  0.09293696 0.07604873\n",
      " 0.1336    ] \n",
      " Balance 949.7748806354036\n",
      "Act: 1 \n",
      " Obs: [0.9634993  0.9809248  0.9484214  0.8948521  0.9177361  0.97815603\n",
      " 0.99794745 0.95031995 0.94084823 0.9526498  0.9592584  0.96955997\n",
      " 0.9610963  0.89392686 0.929588   0.88909996 0.9157731  0.9394618\n",
      " 0.98454905 0.95601743 0.9063338  0.93606067 0.95009947 0.91686004\n",
      " 0.10983434 0.04207152 0.04107323 0.57108706 0.06590144 0.\n",
      " 0.1368    ] \n",
      " Balance 1529.4394942791862\n",
      "Act: 1 \n",
      " Obs: [0.9483016  0.9670877  0.9330485  0.898819   0.9055777  0.9678885\n",
      " 0.97761977 0.93803096 0.93227535 0.9348593  0.94826454 0.95512635\n",
      " 0.9377264  0.8871137  0.9258887  0.9924407  0.89837676 0.92779344\n",
      " 0.9728134  0.9451508  0.8918613  0.92522395 0.94390833 0.91225404\n",
      " 0.11019813 0.03363205 0.07602565 0.56946665 0.06698449 0.\n",
      " 0.9288    ] \n",
      " Balance 1642.1572253391405\n",
      "Act: 4 \n",
      " Obs: [0.9368994  0.9627754  0.916609   1.0013274  0.89407086 0.95489484\n",
      " 0.9633136  0.92683613 0.91717213 0.9250872  0.9251594  0.93997025\n",
      " 0.9289183  1.0002288  0.9146217  0.9690141  1.0025158  0.9176952\n",
      " 0.95550424 0.93707097 1.0048509  0.9023107  0.9292881  0.9105177\n",
      " 0.13504967 0.04121665 0.13812822 0.51394975 0.13299784 0.\n",
      " 0.0152    ] \n",
      " Balance 890.3404174180982\n",
      "Act: 1 \n",
      " Obs: [0.9299922  0.9499109  0.90906024 0.99682856 1.0104346  0.9346675\n",
      " 0.9409741  0.9143583  0.9019747  0.9169     0.9120895  0.92545456\n",
      " 0.91869867 0.9864659  0.9018668  0.9654691  0.99789274 0.90338385\n",
      " 0.9453975  0.9248     0.98188025 0.89667404 0.91543025 0.8972186\n",
      " 0.14183883 0.04328867 0.13812873 0.5064812  0.069258   0.\n",
      " 0.0344    ] \n",
      " Balance 1743.214934666042\n",
      "Act: 1 \n",
      " Obs: [0.92655426 0.94064623 0.9034708  0.98114413 1.0012089  0.9322793\n",
      " 0.9284936  0.9077178  0.8910838  0.9072404  0.901143   0.91539264\n",
      " 0.90711737 0.98401344 0.8957346  0.94560874 0.98844326 0.8954661\n",
      " 0.93706214 0.9165231  0.97746134 1.0193481  0.9043004  0.89017534\n",
      " 0.1448572  0.04420987 0.16420986 0.49317336 0.07051323 0.\n",
      " 0.3664    ] \n",
      " Balance 1773.1536858645768\n",
      "Act: 1 \n",
      " Obs: [0.91080546 0.9297984  0.90872645 0.96863776 0.9975972  0.9123528\n",
      " 0.91501415 0.89558184 0.8758262  0.8926482  0.8901455  0.90447265\n",
      " 0.8928792  0.98391926 0.8851567  0.93233496 0.9738504  0.88316554\n",
      " 0.92648894 0.9198868  0.96316516 1.0102067  0.8912413  1.0106575\n",
      " 0.18209831 0.05557572 0.16420986 0.4847201  0.07177961 0.\n",
      " 0.14      ] \n",
      " Balance 1779.3282556174663\n",
      "Act: 2 \n",
      " Obs: [0.89807427 0.9164717  0.89748055 0.95728123 0.99585795 0.9082951\n",
      " 0.90013206 0.88811904 1.0036814  0.8830705  0.8880513  0.89312845\n",
      " 0.88829756 0.96852064 0.87255543 0.9228805  0.9694412  1.0108367\n",
      " 0.9153479  0.9086763  0.95677793 0.99997276 0.87949765 0.99807036\n",
      " 0.18525091 0.05653788 0.16974749 0.50560796 0.0941653  0.\n",
      " 0.4184    ] \n",
      " Balance 1910.0353453395373\n",
      "Act: 2 \n",
      " Obs: [0.88759315 0.9037864  0.8889607  0.95514446 0.9795166  0.90302086\n",
      " 0.9062131  0.87677157 0.9899941  0.87285024 0.8692387  0.87159914\n",
      " 0.8695483  0.9513401  0.99974644 0.9051399  0.95826167 1.0026973\n",
      " 0.9083333  0.89072895 0.94477403 0.9927929  1.0106494  0.99034625\n",
      " 0.18813035 0.05741667 0.18077813 0.5319831  0.09611496 0.\n",
      " 0.8824    ] \n",
      " Balance 1928.844353431066\n",
      "Act: 2 \n",
      " Obs: [0.87070125 0.88828903 0.88046306 0.9504382  0.9815592  0.88919514\n",
      " 0.90166485 0.86697906 0.98415935 0.85995626 1.0156606  0.8621396\n",
      " 1.0077194  0.9381767  0.9783532  0.9079018  0.9367571  1.0012172\n",
      " 0.8954606  0.8819654  0.93148977 0.99035794 0.9977789  0.9875441\n",
      " 0.19757754 0.06029992 0.18077831 0.45912147 0.09816764 0.\n",
      " 0.5504    ] \n",
      " Balance 1939.5120652570542\n",
      "Act: 1 \n",
      " Obs: [0.85413975 0.8786904  0.87193966 0.94128567 0.97761905 0.8996593\n",
      " 0.89375436 0.86664164 0.97681445 1.0075452  1.0014246  0.8529574\n",
      " 0.9910866  0.93084013 0.9683888  0.         0.9196819  0.9946947\n",
      " 0.88619834 0.8704636  0.92444307 0.9849563  0.98276585 0.9818179\n",
      " 0.22994629 0.07017874 0.18282528 0.46074983 0.07716084 0.\n",
      " 0.412     ] \n",
      " Balance 2226.928725784337\n",
      "Act: 1 \n",
      " Obs: [0.84943783 0.86565405 0.85494304 0.9277448  0.9725182  0.8932805\n",
      " 0.8803917  0.85214496 0.96076244 0.99489427 0.98490196 0.8448659\n",
      " 0.9725446  0.9207617  0.96197873 1.0102568  0.91260237 0.98596644\n",
      " 0.8853277  0.8528566  0.91085726 0.97572106 0.97963285 0.97616744\n",
      " 0.24595335 0.07506403 0.28252518 0.47136122 0.10230409 0.\n",
      " 0.18      ] \n",
      " Balance 2263.9147279586086\n",
      "Act: 2 \n",
      " Obs: [1.0363737  0.85582274 0.8512335  0.9147751  0.9714938  0.8818263\n",
      " 0.8662651  0.830817   0.94536656 0.9768403  0.97530574 1.0425062\n",
      " 0.96955717 0.9167655  0.95158786 0.9972227  0.90009344 0.9691526\n",
      " 0.8756043  0.84509134 0.9009377  0.97060424 0.967935   0.95568097\n",
      " 0.2605994  0.07953396 0.2825252  0.4092348  0.11096194 0.\n",
      " 0.1536    ] \n",
      " Balance 2352.4246379543447\n",
      "Act: 1 \n",
      " Obs: [1.023883   0.85001147 0.84165406 0.90085334 0.957925   0.8753978\n",
      " 0.8526361  1.0354875  0.9335544  0.9660458  0.9646987  1.0492685\n",
      " 0.9597043  0.90180993 0.94130355 0.9808908  0.8923459  0.9573265\n",
      " 0.85747576 0.8250363  0.8862272  0.95620024 0.95951843 0.94000435\n",
      " 0.28436795 0.08678803 0.2825252  0.38581574 0.08141283 0.\n",
      " 0.2944    ] \n",
      " Balance 2778.618799318506\n",
      "Act: 2 \n",
      " Obs: [1.0148448  0.8292031  1.0292662  0.8901795  0.948775   0.8620396\n",
      " 0.84160656 1.0288062  0.9162861  0.9575202  0.9568193  1.0403404\n",
      " 0.95126635 0.8931274  0.92973226 0.96195257 0.8803713  0.96188396\n",
      " 0.84876364 1.037806   0.880958   0.9389993  0.95191026 0.92391086\n",
      " 0.26504636 0.08089115 0.28879738 0.38479865 0.10593142 0.\n",
      " 0.7856    ] \n",
      " Balance 2788.124825333515\n",
      "Act: 1 \n",
      " Obs: [1.0064206  1.0329106  1.0197661  0.8777867  0.9410778  0.85783625\n",
      " 0.84016675 1.0134366  0.90313035 0.95359784 0.9335081  1.0270021\n",
      " 0.9357639  0.884642   0.9196825  0.9429521  0.86502486 0.9471668\n",
      " 0.842297   1.027471   0.8768915  0.920328   0.9389339  0.90814304\n",
      " 0.34865582 0.10640845 0.28879738 0.37200037 0.08440842 0.\n",
      " 0.3904    ] \n",
      " Balance 2812.5365274745113\n",
      "Act: 2 \n",
      " Obs: [0.9968674  1.0233434  1.0128655  0.86211956 0.93580055 0.8475305\n",
      " 1.03393    1.0002216  0.8946004  0.9460096  0.9250017  1.0135971\n",
      " 0.91945964 0.86524993 0.90635717 0.9328302  0.86082083 0.9342657\n",
      " 1.0326473  1.0215645  0.8676754  0.90661556 0.9169333  0.9026323\n",
      " 0.30265328 0.09236864 0.28879738 0.34739554 0.10975406 0.\n",
      " 0.824     ] \n",
      " Balance 3382.2610960459615\n",
      "Act: 5 \n",
      " Obs: [0.9874567  1.0126082  0.99612457 1.0339576  0.9315389  1.025369\n",
      " 1.0203562  0.9800379  0.8804229  0.9279206  0.92259157 0.99356693\n",
      " 0.90825105 1.0305207  0.901045   0.9240369  1.0337586  0.\n",
      " 1.0254498  1.0156585  1.0347673  0.9043691  0.9090624  0.8856208\n",
      " 0.35876018 0.10949225 0.29254502 0.34646842 0.17886198 0.\n",
      " 0.7552    ] \n",
      " Balance 2397.654362692555\n",
      "Act: 0 \n",
      " Obs: [0.9697684  1.0056604  0.9829626  1.0270052  0.92658293 1.0146888\n",
      " 1.0134263  0.96408033 0.86364555 0.91502774 0.9221052  0.9817934\n",
      " 0.9014586  1.0303715  0.88623565 0.91806996 1.0220178  0.\n",
      " 1.0147697  1.002886   1.0237784  0.8926114  0.89950645 0.8762761\n",
      " 0.33154726 0.10118698 0.29254583 0.3441164  0.05973665 0.\n",
      " 0.644     ] \n",
      " Balance 3845.268855176275\n",
      "Act: 0 \n",
      " Obs: [0.9529786  0.99016213 0.97092354 1.0150317  0.91784394 1.0020524\n",
      " 1.0084988  0.9573844  0.8592565  0.90433717 0.909034   0.97447914\n",
      " 0.892932   1.0183362  0.8870525  0.9072903  1.0143495  0.\n",
      " 1.0037078  0.98186934 1.0090767  0.         0.88931024 0.87020063\n",
      " 0.39055234 0.1191951  0.29254583 0.36268532 0.06093139 0.\n",
      " 0.348     ] \n",
      " Balance 3528.1740564274533\n",
      "Act: 0 \n",
      " Obs: [0.947183   0.98200756 0.95600486 1.0011257  0.9126527  0.982512\n",
      " 1.0002253  0.93950355 0.8573953  0.90791875 0.8908264  0.96260625\n",
      " 0.88289005 1.0096498  0.88302445 0.9005956  0.9982752  0.\n",
      " 1.0001295  0.9805163  0.99086046 0.         0.8772996  0.85589415\n",
      " 0.3983332  0.12156979 0.293019   0.3530706  0.05944784 0.\n",
      " 0.6712    ] \n",
      " Balance 3795.363189103803\n",
      "Act: 8 \n",
      " Obs: [0.94017863 0.9622266  0.94642293 0.9814183  0.8964538  0.9646598\n",
      " 0.99477    0.9243765  1.0410933  0.8952083  1.0318507  0.9552847\n",
      " 1.0337952  0.9965747  1.0381533  0.8854847  0.9867753  1.0324008\n",
      " 0.99734104 0.97816956 0.98739094 1.039977   1.030799   1.03187\n",
      " 0.3983332  0.12156979 0.293019   0.3530706  0.27670488 0.01320262\n",
      " 0.9296    ] \n",
      " Balance 2461.0126866457003\n",
      "total financial balance: (eur) 53738.65525473102 internal rate of return 28.65155125224854 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate1(1, env_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b1e177",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate2(episodes, environment, model):\n",
    "    \n",
    "    mean_irr = 0\n",
    "    mean_fin_balance = 0\n",
    "    irr = 0\n",
    "    fin_balance = 0\n",
    "    count = 0\n",
    "    npv = 0\n",
    "    list_npv = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "\n",
    "        obs, _ = environment.reset()  # Unpack the tuple and ignore the info part\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)  # Now obs is just the observation array\n",
    "            obs, reward, done, truncated, info = environment.step(action)\n",
    "            # Extracting the 2nd and 3rd key-value pairs\n",
    "            keys = list(info.keys())\n",
    "            values = list(info.values())\n",
    "\n",
    "            # Getting the 2nd key-value pair\n",
    "            second_value = values[1]\n",
    "\n",
    "            # Getting the 3rd key-value pair\n",
    "    \n",
    "            third_value = values[2]\n",
    "            fourth_value = values[4]\n",
    "        \n",
    "        fin_balance += second_value\n",
    "        npv += fourth_value\n",
    "        count += 1\n",
    "        \n",
    "        list_npv.append(fourth_value)\n",
    "            \n",
    "    mean_fin_balance = fin_balance/count\n",
    "    mean_npv = npv/count\n",
    "\n",
    "    #print(mean_npv)\n",
    "\n",
    "    environment.close()\n",
    "    \n",
    "    return(list_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0af1d69",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def basepolicy1(episodes, environment):\n",
    "    \n",
    "    mean_irr = 0\n",
    "    mean_fin_balance = 0\n",
    "    irr = 0\n",
    "    fin_balance = 0\n",
    "    count = 0\n",
    "    irr_count = 0\n",
    "    npv = 0\n",
    "    list_npv = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "\n",
    "        obs, _ = environment.reset()  # Unpack the tuple and ignore the info part\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            action = 0\n",
    "            for i, n in enumerate(obs):\n",
    "                if i < 24:\n",
    "                    if n < 0.75:\n",
    "                        action += 1\n",
    "\n",
    "            obs, reward, done, truncated, info = environment.step(action)\n",
    "\n",
    "            # Extracting the 2nd and 3rd key-value pairs\n",
    "            keys = list(info.keys())\n",
    "            values = list(info.values())\n",
    "\n",
    "            # Getting the 2nd key-value pair\n",
    "            second_value = values[1]\n",
    "\n",
    "            # Getting the 3rd key-value pair\n",
    "    \n",
    "            third_value = values[2]\n",
    "            fourth_value = values[4]\n",
    "        \n",
    "        fin_balance += second_value\n",
    "        npv += fourth_value\n",
    "        count += 1\n",
    "        \n",
    "        list_npv.append(fourth_value)\n",
    "            \n",
    "    mean_fin_balance = fin_balance/count\n",
    "    mean_npv = npv/count\n",
    "\n",
    "    #print(mean_npv, \"\\n\", mean_irr, \"\\n\" )\n",
    "\n",
    "    environment.close()\n",
    "    \n",
    "    return(list_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "77640e7e",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubaw\\AppData\\Local\\Temp\\ipykernel_24352\\2392353474.py:197: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    }
   ],
   "source": [
    "values_policy = basepolicy1(1000, env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a9cad6af",
   "metadata": {
    "code_folding": [
     0,
     47,
     154,
     162,
     228,
     257,
     281,
     416,
     431
    ]
   },
   "outputs": [],
   "source": [
    "class TestEnvironment(gym.Env):\n",
    "    def __init__(self, AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_tariff, efficency, CAPEX, seed):\n",
    "        \n",
    "        # Price per watthour\n",
    "        self.import_price_df = import_tariff\n",
    "        self.import_price_at_zero = np.float32(0.00035)\n",
    "        self.import_price_rate = import_price_rate\n",
    "        \n",
    "        # Energy Balance\n",
    "        self.AC_OUTPUT = AC_OUTPUT_arr\n",
    "        self.elec_df = elec_consum_arr\n",
    "        self.max_export = 4000\n",
    "        self.number_of_panels = 24\n",
    "        \n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)  # Seed numpy's RNG for deterministic results\n",
    "        \n",
    "        # Degradation\n",
    "        self.deg_mu = 1.055\n",
    "        self.deg_std = 0.555\n",
    "        \n",
    "        # Efficency Development\n",
    "        self.efficency_develop_df = efficency\n",
    "        self.efficency_at_zero = 1.0\n",
    "        \n",
    "        # Costs\n",
    "        self.power_at_zero = 415\n",
    "        self.cost_per_Wp_df_at_zero = 0.69\n",
    "        self.cost_per_Wp_df = CAPEX\n",
    "        self.initial_other_costs = 150\n",
    "        \n",
    "        self.operational_cost = 16.8\n",
    "        #self.budget_constraint = 1500\n",
    "        \n",
    "        self.loan_interest_rate = 1.06\n",
    "        self.normal_interest_rate = 1.02\n",
    "                        \n",
    "        # Spaces and length\n",
    "        self.action_space = spaces.Discrete(self.number_of_panels + 1)\n",
    "        self.observation_space = spaces.Box(0, 1.25, shape=(self.number_of_panels + 7,))\n",
    "        self.episode_len = 25\n",
    "        self.months_per_timestep = 12\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        \n",
    "        return self.observation\n",
    "    \n",
    "    def calculate_import_export(self, AC_OUTPUT, elec_df, export_price, import_price):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate the annual Wh of energy exported to the grid (exported) and saved (minimised)\n",
    "        \"\"\"\n",
    "        \n",
    "        AC_OUTPUT_tot = self._get_obs()[0:self.number_of_panels].sum() * self.AC_OUTPUT \n",
    "\n",
    "        exported = (AC_OUTPUT_tot - self.elec_df).clip(min=0, max = self.max_export)        \n",
    "        export_revenue = (export_price * exported).sum()\n",
    "\n",
    "        \n",
    "        minimised = AC_OUTPUT_tot - exported \n",
    "        minimised_revenue = (minimised * (self.import_price_rate * import_price)).sum()\n",
    "        \n",
    "\n",
    "        return export_revenue, AC_OUTPUT_tot, minimised_revenue\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reset the environment to the original state at t=1\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "                \n",
    "        # Panels\n",
    "        self.init_obs = np.random.choice([1, 0], size=self.number_of_panels, p=[0.4, 0.6]).astype(np.float32)\n",
    "        self.init_obs = np.where(self.init_obs < 0.5, 0.0, np.random.uniform(0.85, 1.0, size=self.number_of_panels))\n",
    "\n",
    "        # Combine all initialization into a single step for efficiency\n",
    "        self.import_price_at_zero_norm = (self.import_price_at_zero - 0.00022499) / (0.0020798 - 0.00022499)\n",
    "        self.FiT_at_zero_norm = (self.import_price_at_zero - 0.00022499 * 0.33) / (0.0020798 - 0.00022499 * 0.33)\n",
    "        self.efficency_at_zero_norm = (self.efficency_at_zero - 0.999) / (1.156 - 0.999)\n",
    "        self.panel_cost_and_inverter_at_zero_norm = (self.cost_per_Wp_df_at_zero - 0.254) / (1.12 - 0.254)\n",
    "        \n",
    "        self.current_budget_constraint = np.random.randint(750, 2000)\n",
    "        self.next_step_budget_constraint = 0\n",
    "        \n",
    "        \n",
    "        # Complete observation initialization in one go\n",
    "        self.observation = np.concatenate([\n",
    "            self.init_obs,\n",
    "            [self.import_price_at_zero_norm, self.FiT_at_zero_norm, self.efficency_at_zero_norm, \n",
    "             self.panel_cost_and_inverter_at_zero_norm, 0., 0., 0.]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "        self.previous_observation = self.observation.copy()\n",
    "\n",
    "        # RANDOM IMPORT PRICE\n",
    "        self.random_import_price = self.import_price_df[np.random.choice(self.import_price_df.shape[0])] \n",
    "\n",
    "        # RANDOM EFFICENCY\n",
    "        self.random_efficency_develop = self.efficency_develop_df[np.random.choice(self.efficency_develop_df.shape[0])]   \n",
    "        \n",
    "        # RANDOM COST PER WP\n",
    "        self.random_cost_per_Wp = self.cost_per_Wp_df[np.random.choice(self.cost_per_Wp_df.shape[0])]   \n",
    "        \n",
    "        \n",
    "        self.episode_len = 25  \n",
    "    \n",
    "        info = {}\n",
    "        \n",
    "        # RESET BALANCES\n",
    "        self.fin_balance_tot = 0\n",
    "        self.reward_tot = 0\n",
    "        self.env_balance_tot = 0\n",
    "        self.produced = 0\n",
    "        self.other_costs = 0\n",
    "        self.FiT = 0.0004\n",
    "        self.next_FiT = 0.0004\n",
    "        self.resale_values = array_of_zeros = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        \n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "        self.total_cash_flow = []\n",
    "        self.annual_cash_flow = 0\n",
    "                \n",
    "        self.due_loans = [0, 0, 0, 0] \n",
    "        self.current_interest = 0\n",
    "        self.step_total_interest = 1\n",
    "        \n",
    "        self.two_year_ago_interest = 0\n",
    "        self.first_year_interest = []\n",
    "        self.second_year_interest = [0]\n",
    "        self.third_year_interest = [0, 0]\n",
    "        self.fourth_year_interest = [0, 0, 0]\n",
    "        self.next_year_total = 0\n",
    "        \n",
    "        self.survival = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "    \n",
    "        return self.observation, info\n",
    "    \n",
    "    def calculate_resale(self, initial_panel_cost, indices):\n",
    "        \n",
    "        self.resale_values[indices] = initial_panel_cost\n",
    "        \n",
    "        self.resale_values = self.resale_values * 0.85\n",
    "        \n",
    "        for count, i in enumerate(self.broke):\n",
    "            if i == 1:\n",
    "                self.resale_values[count] = 0\n",
    "        \n",
    "        resale_step = self.resale_values[indices].sum()\n",
    "        \n",
    "        return resale_step\n",
    "    \n",
    "    def calculate_panel_inv_cost(self, cost_per_Wp):\n",
    "        \n",
    "        PW_ep = self.efficency_develop * self.power_at_zero\n",
    "        \n",
    "        panel_cost_and_inverter = PW_ep * cost_per_Wp\n",
    "        \n",
    "        return panel_cost_and_inverter\n",
    "    \n",
    "    def calculate_irr_and_npv(self, pv_cost, minimised_revenue, export_revenue, penalty):\n",
    "                \n",
    "        \"\"\"\n",
    "        Calculates total cash flow of the project needed for the internal rate of return\n",
    "        \"\"\" \n",
    "        self.expences = 0\n",
    "        self.annual_cash_flow = 0\n",
    "        initial_cost = 0\n",
    "        \n",
    "        self.expences = pv_cost\n",
    "        self.annual_cash_flow = self.expences + export_revenue + minimised_revenue + penalty\n",
    "        initial_cost_q, x = self.calculate_total_CAPEX(self.init_obs, self.panel_cost_and_inverter)\n",
    "        initial_cost = - initial_cost_q\n",
    "        \n",
    "        if self.episode_len == 24:\n",
    "            self.total_cash_flow.append(initial_cost + self.annual_cash_flow) \n",
    "        else:\n",
    "            self.total_cash_flow.append(self.annual_cash_flow) \n",
    "        \n",
    "        return self.total_cash_flow\n",
    "        \n",
    "    def calculate_penalty(self, current_step, annual_expense):\n",
    "              \n",
    "        year = 25 - current_step\n",
    "        \n",
    "        if year > 0:\n",
    "            self.current_budget_constraint = self.next_step_budget_constraint    \n",
    "            \n",
    "        \n",
    "        self.current_interest = self.next_year_total\n",
    "        annual_expense = (-annual_expense)\n",
    "        value = 0 \n",
    "        loan = 0\n",
    "        annual_interest = 0\n",
    "\n",
    "        if annual_expense > self.current_budget_constraint:\n",
    "            loan = (self.current_budget_constraint - annual_expense)\n",
    "            value = annual_expense / self.current_budget_constraint\n",
    "            periods = 2 if value < 2 else 3 if value < 3 else 4\n",
    "\n",
    "            annual_interest = loan / periods\n",
    "            interest_multiplier = 1\n",
    "\n",
    "            for i in range(4):\n",
    "                if i < periods:\n",
    "                    self.due_loans[i] = annual_interest * interest_multiplier\n",
    "                    interest_multiplier *= self.loan_interest_rate\n",
    "                else:\n",
    "                    self.due_loans[i] = 0\n",
    "        else:\n",
    "             self.due_loans = [0, 0, 0, 0]\n",
    "    \n",
    "        self.first_year_interest.append(self.due_loans[0])\n",
    "        self.second_year_interest.append(self.due_loans[1])\n",
    "        self.third_year_interest.append(self.due_loans[2])\n",
    "        self.fourth_year_interest.append(self.due_loans[3])\n",
    "    \n",
    "    \n",
    "        self.next_year_total = self.first_year_interest[year] + self.second_year_interest[year] + self.third_year_interest[year] + self.fourth_year_interest[year]\n",
    "        \n",
    "        self.next_step_budget_constraint = np.random.randint(750, 2000) * self.step_total_interest\n",
    "        current_budget_observation = (self.next_step_budget_constraint - 750 * self.step_total_interest) / (2000 * self.step_total_interest - 750 * self.step_total_interest) \n",
    "        self.observation[self.number_of_panels + 6] = current_budget_observation\n",
    "                \n",
    "        return self.current_interest, self.due_loans, self.next_year_total\n",
    "        \n",
    "    def calculate_total_CAPEX(self, action_step, panel_cost_and_inverter):\n",
    "        \"\"\"\n",
    "        Calculate CAPEX each step in a vectorized manner.\n",
    "        \"\"\"\n",
    "        BOS = panel_cost_and_inverter * 0.55\n",
    "        number_installed = int(np.sum(action_step))\n",
    "\n",
    "        # Calculate costs from module and inverter\n",
    "        panel_cost_and_inverter_step = panel_cost_and_inverter * number_installed\n",
    "\n",
    "        # Calculate other installation costs\n",
    "        if number_installed == 0:\n",
    "            other_costs = 0\n",
    "        elif number_installed == 1:\n",
    "            other_costs = self.initial_other_costs * self.step_total_interest\n",
    "        else:\n",
    "            discounts = 0.9 ** np.arange(number_installed)\n",
    "            other_costs = (self.initial_other_costs * self.step_total_interest * discounts).sum()\n",
    "\n",
    "        # Calculate BOS costs using vector operations\n",
    "        is_new_installation = (self.previous_observation[:number_installed] == 0) & (action_step[:number_installed] == 1)\n",
    "        is_replacement = (self.previous_observation[:number_installed] > 0) & (action_step[:number_installed] == 1)\n",
    "        BOS_cost = np.sum(BOS * is_new_installation) + np.sum((BOS / 2) * is_replacement)\n",
    "\n",
    "        # Sum total CAPEX\n",
    "        total_CAPEX = panel_cost_and_inverter_step + BOS_cost + other_costs\n",
    "\n",
    "        return total_CAPEX, panel_cost_and_inverter\n",
    "        \n",
    "    def failure(self, actions):\n",
    "        \n",
    "        beta = 3  # Shape parameter\n",
    "        phi = 30  # Scale parameter  \n",
    "\n",
    "        # Determine which panels are active based on the actions and previous observations.\n",
    "        if self.episode_len == 24:\n",
    "            active_panels = (self.observation[:self.number_of_panels] > 0.85)\n",
    "        else:\n",
    "            active_panels = (self.observation[:self.number_of_panels] == self.efficency_develop)\n",
    "\n",
    "        # Calculate lifespan for all active panels at once\n",
    "        lifespans = np.random.weibull(beta, self.number_of_panels) * phi\n",
    "        lifespans = np.where(active_panels, lifespans, 0)  # Apply lifespan only to active panels\n",
    "\n",
    "        # Adjust survival times based on episode length\n",
    "        self.survival[:self.number_of_panels] = np.where(\n",
    "            active_panels,\n",
    "            np.abs(lifespans.astype(int)) + np.abs(self.episode_len - 25),\n",
    "            self.survival[:self.number_of_panels]\n",
    "        )\n",
    "\n",
    "        return self.survival\n",
    "\n",
    "    def calculate_FiT(self, episodes, import_price):\n",
    "            \n",
    "        self.FiT = import_price\n",
    "            \n",
    "        if episodes == 25:\n",
    "            self.FiT = self.FiT\n",
    "            \n",
    "        elif episodes == 24 or episodes == 23:\n",
    "            self.FiT = self.FiT * 0.64\n",
    "            \n",
    "        elif episodes == 22:\n",
    "            self.FiT = self.FiT * 0.46\n",
    "            \n",
    "        elif episodes == 21:\n",
    "            self.FiT = self.FiT * 0.55\n",
    "            \n",
    "        elif episodes < 20:\n",
    "            self.FiT = self.FiT * 0.33\n",
    "            \n",
    "        elif episodes == 20:\n",
    "            self.FiT = self.FiT * 0.37\n",
    "            \n",
    "        return self.FiT\n",
    "                        \n",
    "    def step(self, action):\n",
    "        \n",
    "        \"\"\"\n",
    "        defines actions, reward etc.\n",
    "        \"\"\"\n",
    "        \n",
    "        # RESET THE ANNUAL BALANCES\n",
    "        self.total_CAPEX = 0\n",
    "        self.pv_costs = 0\n",
    "        self.fin_balance = 0\n",
    "        self.number_installed = 0\n",
    "        irr_fin = 0\n",
    "        npv_fin = 0\n",
    "        current_penalty = 0\n",
    "        self.other_costs = 0\n",
    "        next_step_penalty = 0\n",
    "        self.step_total_interest = self.step_total_interest * self.normal_interest_rate\n",
    "        current_operational_costs = self.operational_cost * self.step_total_interest\n",
    "        \n",
    "        \n",
    "        self.cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "        self.import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "        self.efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "           \n",
    "        self.panel_cost_and_inverter = self.calculate_panel_inv_cost(self.cost_per_Wp)\n",
    "        FiT = self.calculate_FiT(self.episode_len, self.import_price)\n",
    "        \n",
    "        reward = 0   \n",
    "        actions_step = np.random.rand(self.number_of_panels + 1)\n",
    "        \n",
    "        \n",
    "        # Find indices of the lowest 'action' values in previous_observation\n",
    "        indices = np.argsort(self.previous_observation[:self.number_of_panels])[:action]\n",
    "\n",
    "        # Replace these indices in the observation with efficiency_develop\n",
    "        self.observation[:self.number_of_panels][indices] = self.efficency_develop\n",
    "\n",
    "        # Copy over the other values from previous_observation to observation\n",
    "        mask = np.ones(len(self.previous_observation[:self.number_of_panels]), dtype=bool)\n",
    "        mask[indices] = False\n",
    "        self.observation[:self.number_of_panels][mask] = self.previous_observation[:self.number_of_panels][mask]\n",
    "\n",
    "        replaced_panels = np.zeros(len(self.previous_observation[:self.number_of_panels]), dtype=int)\n",
    "        replaced_panels[indices] = 1\n",
    "\n",
    "        instaltion = (self.observation[:self.number_of_panels] > 0).astype(int)\n",
    "        self.pv_costs -= instaltion.sum() * current_operational_costs\n",
    "\n",
    "        actions_step = np.array(replaced_panels)\n",
    "\n",
    "            \n",
    "        if action > 0:\n",
    "            step_CAPEX, panel_cost_and_inverter = self.calculate_total_CAPEX(actions_step, self.panel_cost_and_inverter)\n",
    "            self.pv_costs -= step_CAPEX\n",
    "            \n",
    "        else:\n",
    "            panel_cost_and_inverter = 0\n",
    "                \n",
    "        next_observation = self._get_obs()\n",
    "\n",
    "        # Calculate the Reslae value\n",
    "        resale = self.calculate_resale(panel_cost_and_inverter, indices) #  ***\n",
    "        \n",
    "        self.pv_costs += resale\n",
    "\n",
    "        \n",
    "        # CALCULATE THE BUDGET INTEREST\n",
    "        current_penalty, due_loans, next_step_penalty = self.calculate_penalty(self.episode_len, self.pv_costs)\n",
    "        \n",
    "        \n",
    "        # CALCULATE THE ENERGY YIELD\n",
    "        exported_revenue, AC_OUTPUT_tot, minimised_revenue = self.calculate_import_export(self.AC_OUTPUT, \n",
    "                                                                          self.elec_df, FiT, self.import_price)        \n",
    "        \n",
    "        pv_costs_observation = - self.pv_costs / 10000\n",
    "        self.observation[self.number_of_panels + 4] = pv_costs_observation\n",
    "        \n",
    "        next_step_penalty_observation = - next_step_penalty / 8000\n",
    "        self.observation[self.number_of_panels + 5] = next_step_penalty_observation\n",
    "        \n",
    "        \n",
    "        # CALCULATE STEP BALANCES\n",
    "        self.fin_balance += self.pv_costs\n",
    "        self.fin_balance += current_penalty\n",
    "        self.fin_balance += float(exported_revenue + minimised_revenue)\n",
    "        \n",
    "        # CALCULATE TOTAL BALANCES\n",
    "        self.fin_balance_tot += self.fin_balance                \n",
    "        \n",
    "        # SUBSTRACT 1 FOR TIMESTEP\n",
    "        self.episode_len -= 1\n",
    "        done = self.episode_len <= 0\n",
    "        \n",
    "        # CALCULATE IRR, NPV AND CARBON INTENSITY\n",
    "        total_cash_flow = self.calculate_irr_and_npv(self.pv_costs, exported_revenue, minimised_revenue, current_penalty)\n",
    "        irr = npf.irr(total_cash_flow) * 100\n",
    "        npv = npf.npv(0.04 ,total_cash_flow)\n",
    "            \n",
    "        # RETURNS AND CALCULATE REWARD\n",
    "        if self.episode_len == 0:\n",
    "            irr_fin = irr\n",
    "            npv_fin = npv\n",
    "        \n",
    "        reward = self.fin_balance / 1000\n",
    "        #reward = self.fin_balance_tot / 1000 if done else 0\n",
    "        \n",
    "        # FAILURE\n",
    "         \n",
    "        survival = self.failure(actions_step)\n",
    "        self.broke = np.zeros(self.number_of_panels, dtype=np.float32)\n",
    "\n",
    "        for c, p in enumerate(survival):\n",
    "            \n",
    "            if c < self.number_of_panels:\n",
    "\n",
    "                if p - 1 <= abs(self.episode_len - 24):\n",
    "                    self.broke[c] = 1\n",
    "\n",
    "                    self.observation[c] = 0\n",
    "        \n",
    "        # DEGRADATION RATE\n",
    "        # Applying degradation only to panels that are operational (above 0.1 efficiency)\n",
    "        active_panels = self.observation[:self.number_of_panels] > 0.1\n",
    "        degradations = np.random.normal(self.deg_mu, self.deg_std, size=self.number_of_panels) / 100\n",
    "        self.observation[:self.number_of_panels][active_panels] -= degradations[active_panels]\n",
    "        \n",
    "        if not done: \n",
    "        \n",
    "            self.next_cost_per_Wp = self.random_cost_per_Wp[abs(self.episode_len - 25)]\n",
    "            self.next_import_price = self.random_import_price[abs(self.episode_len - 25)]\n",
    "            self.next_efficency_develop = self.random_efficency_develop[abs(self.episode_len - 25)]\n",
    "            next_FIT = self.calculate_FiT(self.episode_len, self.next_import_price)\n",
    "        \n",
    "            price_observation = (self.next_import_price - 0.00022499) / (0.0020798 - 0.00022499)\n",
    "            self.observation[self.number_of_panels] = price_observation\n",
    "\n",
    "            FiT_observation = (next_FIT - 0.00022499 * 0.33) / (0.0020798 - 0.00022499 * 0.33)\n",
    "            self.observation[self.number_of_panels + 1] = FiT_observation\n",
    "\n",
    "            eff_observation = (self.next_efficency_develop - 0.999) / (1.156 - 0.999)\n",
    "            self.observation[self.number_of_panels + 2] = eff_observation\n",
    "\n",
    "            cost_per_Wp_observation = (self.next_cost_per_Wp - 0.254) / (1.12 - 0.254) \n",
    "            self.observation[self.number_of_panels + 3] = cost_per_Wp_observation\n",
    "        \n",
    "        \n",
    "        info = {\"budget:\": self.current_budget_constraint,\n",
    "               \"import price\": self.import_price,\n",
    "               \"export price\": FiT,\n",
    "               \"PV_panel_price\": self.panel_cost_and_inverter,\n",
    "                \"pv array size\": (self.observation[:self.number_of_panels]).sum() * self.power_at_zero * self.efficency_develop,\n",
    "               \"current_interest\": current_penalty,\n",
    "               \"efficency_develop\": self.efficency_develop,\n",
    "               \"action\": action}\n",
    "         \n",
    "        \n",
    "        self.previous_observation = self.observation.copy()\n",
    "        \n",
    "        return self.observation, reward, done, False, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "83c32a32",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "env_test2 = TestEnvironment(AC_OUTPUT_arr, elec_consum_arr, import_price_rate, import_price_test_arr, \n",
    "                            Eff_test_arr, CAPEX_JA_test_arr, 24)\n",
    "\n",
    "#4790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ad2064b9",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_visual(episodes, environment, model):\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "\n",
    "        obs, _ = environment.reset()  # Unpack the tuple and ignore the info part\n",
    "        done = False\n",
    "        \n",
    "        budget_arr = []\n",
    "        import_price_arr = []\n",
    "        export_price_arr = []\n",
    "        PV_panel_price_arr = []\n",
    "        pv_array_size_arr = []\n",
    "        current_interest_arr = []\n",
    "        efficency_develop_arr = []\n",
    "        action_arr = []\n",
    "        rewards = 0\n",
    "        \n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)  # Now obs is just the observation array\n",
    "            obs, reward, done, truncated, info = environment.step(action)\n",
    "            # Extracting the 2nd and 3rd key-value pairs\n",
    "            values = list(info.values())\n",
    "\n",
    "            # Getting the 2nd key-value pair\n",
    "            budget = values[0]\n",
    "            budget_arr.append(budget)\n",
    "            \n",
    "            import_price = values[1]\n",
    "            import_price_arr.append(import_price)\n",
    "            \n",
    "            export_price = values[2]\n",
    "            export_price_arr.append(export_price)\n",
    "            \n",
    "            PV_panel_price = values[3]\n",
    "            PV_panel_price_arr.append(PV_panel_price)\n",
    "            \n",
    "            Ppv_array_size = values[4]\n",
    "            pv_array_size_arr.append(Ppv_array_size)\n",
    "            \n",
    "            current_interest = values[5]\n",
    "            current_interest_arr.append(current_interest)\n",
    "            \n",
    "            efficency_develop = values[6]\n",
    "            efficency_develop_arr.append(efficency_develop)   \n",
    "            \n",
    "            action = values[7]\n",
    "            action_arr.append(action)\n",
    "            \n",
    "            rewards += reward\n",
    "            \n",
    "\n",
    "    environment.close()          \n",
    "    \n",
    "    print(rewards)\n",
    "    \n",
    "    return budget_arr, import_price_arr, export_price_arr, PV_panel_price_arr, pv_array_size_arr, current_interest_arr, efficency_develop_arr, action_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "58d80b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.97240286590212\n"
     ]
    }
   ],
   "source": [
    "budget_arr, import_price_arr, export_price_arr, PV_panel_price_arr, pv_array_size_arr, current_interest_arr, efficency_develop_arr, action_arr = evaluate_visual(1, env_test2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c5829e19",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def basepolicy2(episodes, environment):\n",
    "    \n",
    "    pv_array_size_arr = []\n",
    "    action_arr = []\n",
    "    current_interest_arr = []\n",
    "    rewards = 0\n",
    "\n",
    "    for ep in range(episodes):\n",
    "\n",
    "        obs, _ = environment.reset()  # Unpack the tuple and ignore the info part\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            action = 0\n",
    "            for i, n in enumerate(obs):\n",
    "                if i < 24:\n",
    "                    if n < 0.8:\n",
    "                        action += 1\n",
    "\n",
    "            obs, reward, done, truncated, info = environment.step(action)\n",
    "\n",
    "            # Extracting the 2nd and 3rd key-value pairs\n",
    "            values = list(info.values())\n",
    "\n",
    "            Ppv_array_size = values[4]\n",
    "            pv_array_size_arr.append(Ppv_array_size)\n",
    "\n",
    "            # Getting the 3rd key-value pair\n",
    "    \n",
    "            action = values[7]\n",
    "            action_arr.append(action)\n",
    "            \n",
    "            current_interest = values[5]\n",
    "            current_interest_arr.append(current_interest)\n",
    "            \n",
    "            rewards += reward\n",
    "\n",
    "    print(rewards)\n",
    "    environment.close()\n",
    "    return pv_array_size_arr, action_arr, current_interest_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c1355399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.72972577650635\n"
     ]
    }
   ],
   "source": [
    "Ppv_array_size_policy, action_arr_policy, current_interest_arr_policy  = basepolicy2(1, env_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f3d22631",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Budget",
         "type": "scatter",
         "xaxis": "x",
         "y": [
          1080,
          1054.68,
          1419.1056,
          2089.518552,
          2090.17650096,
          1342.5622566912,
          1425.721622788224,
          1667.8915894267545,
          1369.6698163916487,
          1142.5084956029293,
          1582.254757153195,
          1735.7505345189347,
          1071.664316405351,
          1261.2664646924513,
          1601.8472183583272,
          1283.95839476122,
          1736.5739169396247,
          1428.2462475762734,
          1446.813448794765,
          1612.6899679882733,
          2786.1513674594153,
          1488.3843497077596,
          1768.6007433676075,
          2354.310601437757,
          1540.8828849972667
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines",
         "name": "Import Price",
         "type": "scatter",
         "xaxis": "x2",
         "y": [
          0.00035,
          0.0003516844934204,
          0.000384612637487,
          0.0003862544868637,
          0.0004056042389029,
          0.0004348969130984,
          0.0004598362142441,
          0.0004900820418929,
          0.0005386213123288,
          0.0006062410713816,
          0.0006098153165368,
          0.0005703902376164,
          0.000569529114136,
          0.0005980110523755,
          0.0006420556019226,
          0.0006592521457458,
          0.0006085947612029,
          0.0005731862443443,
          0.0006239933439267,
          0.0006356327113926,
          0.0006017621230935,
          0.0005530625018875,
          0.0005861174486482,
          0.0006459825414356,
          0.0006935012064814
         ],
         "yaxis": "y2"
        },
        {
         "mode": "lines",
         "name": "Export Price",
         "type": "scatter",
         "xaxis": "x3",
         "y": [
          0.00035,
          0.000225078075789056,
          0.00024615208799168,
          0.00017767706395730202,
          0.00022308233139659502,
          0.000160911857846408,
          0.000151745950700553,
          0.00016172707382465702,
          0.000177745033068504,
          0.00020005955355592803,
          0.000201239054457144,
          0.00018822877841341202,
          0.00018794460766488,
          0.000197343647283915,
          0.000211878348634458,
          0.000217553208096114,
          0.00020083627119695703,
          0.00018915146063361904,
          0.00020591780349581102,
          0.00020975879475955802,
          0.000198581500620855,
          0.000182510625622875,
          0.00019341875805390602,
          0.00021317423867374803,
          0.00022885539813886202
         ],
         "yaxis": "y3"
        },
        {
         "mode": "lines",
         "name": "Panel CAPEX",
         "type": "scatter",
         "xaxis": "x4",
         "y": [
          372.25499999999994,
          378.5105408517854,
          383.44277489708355,
          358.12998051505485,
          373.86622291402625,
          367.8658541140195,
          360.6547928154583,
          346.05698728545644,
          324.35229274001824,
          313.02819985166525,
          312.19967629833764,
          301.1650364804651,
          295.9863854690821,
          295.8273804594916,
          294.2095452489729,
          290.76053850804016,
          290.0241100744337,
          287.09743703370754,
          287.25382397521844,
          268.9396500706438,
          249.15912829895635,
          232.9926861330735,
          217.3417060887987,
          225.754944301904,
          220.2116813549955
         ],
         "yaxis": "y4"
        },
        {
         "mode": "lines",
         "name": "Efficency",
         "type": "scatter",
         "xaxis": "x5",
         "y": [
          1,
          1.0000169864206203,
          1.0000175881111244,
          1.0000185804689026,
          1.001384490387354,
          1.0042806698953273,
          1.0113032490456346,
          1.0113040897596146,
          1.0113372702404113,
          1.0113441183192728,
          1.0113458489428773,
          1.0115526774435488,
          1.0115526822882384,
          1.0115529453912186,
          1.0116396019085896,
          1.0155689855648655,
          1.0155913927242743,
          1.0155913927242886,
          1.0223140297291864,
          1.0223141144903003,
          1.0223144188436217,
          1.0227684111379134,
          1.0227962417341683,
          1.026540286534015,
          1.02654052300942
         ],
         "yaxis": "y5"
        },
        {
         "mode": "lines",
         "name": "Interest",
         "type": "scatter",
         "xaxis": "x6",
         "y": [
          0,
          -660.9815198385544,
          -700.6404110288677,
          -742.6788356905998,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          -105.1246862890448,
          -111.4321674663875,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ],
         "yaxis": "y6"
        },
        {
         "mode": "lines",
         "name": "Interest",
         "type": "scatter",
         "xaxis": "x7",
         "y": [
          0,
          -755.4364036925463,
          -800.7625879140991,
          -848.8083431889451,
          -899.7368437802819,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          -95.845522722336,
          -101.59625408567616,
          0,
          0,
          -172.7321066366477,
          -559.3548493913563,
          -398.8343453379003,
          0,
          0,
          0,
          0
         ],
         "yaxis": "y7"
        },
        {
         "name": "Array Size _agent",
         "type": "bar",
         "xaxis": "x8",
         "y": [
          8820.770034790039,
          9136.826380064844,
          9494.207013664198,
          9438.206922042362,
          9403.947108252642,
          9434.305645649703,
          9463.558418581073,
          9445.88089605254,
          9437.208935747936,
          9387.883393688753,
          9335.75770568338,
          8969.395653809663,
          9278.460456972212,
          9436.121201566497,
          9391.799361755604,
          9057.28075715175,
          9395.966957061815,
          9375.181550213296,
          9415.279511633893,
          9377.480362034948,
          9342.837523247801,
          9033.065254886056,
          9363.253009298582,
          9172.636527208773,
          9487.027826046044
         ],
         "yaxis": "y8"
        },
        {
         "name": "Array Size _policy",
         "type": "bar",
         "xaxis": "x9",
         "y": [
          9642.388315200806,
          9540.761044391957,
          9428.39483246044,
          9328.21667536127,
          9245.169102433145,
          9180.132154976985,
          9157.607754191036,
          9041.216636582263,
          8934.984385041545,
          8834.10456931818,
          8820.71640441594,
          8717.55090475541,
          8252.486133722503,
          8424.679215056445,
          8828.456981824611,
          8534.729344142981,
          8186.502429908157,
          8003.341482638109,
          9256.144666989383,
          9434.424639527899,
          9601.16409713631,
          9154.34238045592,
          9581.692815749262,
          8895.42680110169,
          9647.139095266813
         ],
         "yaxis": "y9"
        }
       ],
       "layout": {
        "height": 1400,
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Line and Bar Plots in Separate Subplots"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Index"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ]
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ]
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          1
         ]
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0,
          1
         ]
        },
        "xaxis6": {
         "anchor": "y6",
         "domain": [
          0,
          1
         ]
        },
        "xaxis7": {
         "anchor": "y7",
         "domain": [
          0,
          1
         ]
        },
        "xaxis8": {
         "anchor": "y8",
         "domain": [
          0,
          1
         ]
        },
        "xaxis9": {
         "anchor": "y9",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.9185185185185185,
          1
         ],
         "title": {
          "text": "Value"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.8037037037037038,
          0.8851851851851853
         ]
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.6888888888888889,
          0.7703703703703704
         ]
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0.5740740740740741,
          0.6555555555555556
         ]
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0.45925925925925926,
          0.5407407407407407
         ]
        },
        "yaxis6": {
         "anchor": "x6",
         "domain": [
          0.34444444444444444,
          0.42592592592592593
         ]
        },
        "yaxis7": {
         "anchor": "x7",
         "domain": [
          0.22962962962962963,
          0.3111111111111111
         ]
        },
        "yaxis8": {
         "anchor": "x8",
         "domain": [
          0.11481481481481481,
          0.1962962962962963
         ],
         "range": [
          6000,
          9495.207013664198
         ]
        },
        "yaxis9": {
         "anchor": "x9",
         "domain": [
          0,
          0.08148148148148149
         ],
         "range": [
          6000,
          9648.139095266813
         ]
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"5105154d-d74a-4060-b4f1-95cbaf7b545f\" class=\"plotly-graph-div\" style=\"height:1400px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5105154d-d74a-4060-b4f1-95cbaf7b545f\")) {                    Plotly.newPlot(                        \"5105154d-d74a-4060-b4f1-95cbaf7b545f\",                        [{\"mode\":\"lines\",\"name\":\"Budget\",\"y\":[1080,1054.68,1419.1056,2089.518552,2090.17650096,1342.5622566912,1425.721622788224,1667.8915894267545,1369.6698163916487,1142.5084956029293,1582.254757153195,1735.7505345189347,1071.664316405351,1261.2664646924513,1601.8472183583272,1283.95839476122,1736.5739169396247,1428.2462475762734,1446.813448794765,1612.6899679882733,2786.1513674594153,1488.3843497077596,1768.6007433676075,2354.310601437757,1540.8828849972667],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Import Price\",\"y\":[0.00035,0.0003516844934204,0.000384612637487,0.0003862544868637,0.0004056042389029,0.0004348969130984,0.0004598362142441,0.0004900820418929,0.0005386213123288,0.0006062410713816,0.0006098153165368,0.0005703902376164,0.000569529114136,0.0005980110523755,0.0006420556019226,0.0006592521457458,0.0006085947612029,0.0005731862443443,0.0006239933439267,0.0006356327113926,0.0006017621230935,0.0005530625018875,0.0005861174486482,0.0006459825414356,0.0006935012064814],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines\",\"name\":\"Export Price\",\"y\":[0.00035,0.000225078075789056,0.00024615208799168,0.00017767706395730202,0.00022308233139659502,0.000160911857846408,0.000151745950700553,0.00016172707382465702,0.000177745033068504,0.00020005955355592803,0.000201239054457144,0.00018822877841341202,0.00018794460766488,0.000197343647283915,0.000211878348634458,0.000217553208096114,0.00020083627119695703,0.00018915146063361904,0.00020591780349581102,0.00020975879475955802,0.000198581500620855,0.000182510625622875,0.00019341875805390602,0.00021317423867374803,0.00022885539813886202],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"mode\":\"lines\",\"name\":\"Panel CAPEX\",\"y\":[372.25499999999994,378.5105408517854,383.44277489708355,358.12998051505485,373.86622291402625,367.8658541140195,360.6547928154583,346.05698728545644,324.35229274001824,313.02819985166525,312.19967629833764,301.1650364804651,295.9863854690821,295.8273804594916,294.2095452489729,290.76053850804016,290.0241100744337,287.09743703370754,287.25382397521844,268.9396500706438,249.15912829895635,232.9926861330735,217.3417060887987,225.754944301904,220.2116813549955],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"mode\":\"lines\",\"name\":\"Efficency\",\"y\":[1.0,1.0000169864206203,1.0000175881111244,1.0000185804689026,1.001384490387354,1.0042806698953273,1.0113032490456346,1.0113040897596146,1.0113372702404113,1.0113441183192728,1.0113458489428773,1.0115526774435488,1.0115526822882384,1.0115529453912186,1.0116396019085896,1.0155689855648655,1.0155913927242743,1.0155913927242886,1.0223140297291864,1.0223141144903003,1.0223144188436217,1.0227684111379134,1.0227962417341683,1.026540286534015,1.02654052300942],\"type\":\"scatter\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"mode\":\"lines\",\"name\":\"Interest\",\"y\":[0,-660.9815198385544,-700.6404110288677,-742.6788356905998,0,0,0,0,0,0,0,0,0,0,-105.1246862890448,-111.4321674663875,0,0,0,0,0,0,0,0,0],\"type\":\"scatter\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"},{\"mode\":\"lines\",\"name\":\"Interest\",\"y\":[0,-755.4364036925463,-800.7625879140991,-848.8083431889451,-899.7368437802819,0,0,0,0,0,0,0,0,0,-95.845522722336,-101.59625408567616,0,0,-172.7321066366477,-559.3548493913563,-398.8343453379003,0,0,0,0],\"type\":\"scatter\",\"xaxis\":\"x7\",\"yaxis\":\"y7\"},{\"name\":\"Array Size _agent\",\"y\":[8820.770034790039,9136.826380064844,9494.207013664198,9438.206922042362,9403.947108252642,9434.305645649703,9463.558418581073,9445.88089605254,9437.208935747936,9387.883393688753,9335.75770568338,8969.395653809663,9278.460456972212,9436.121201566497,9391.799361755604,9057.28075715175,9395.966957061815,9375.181550213296,9415.279511633893,9377.480362034948,9342.837523247801,9033.065254886056,9363.253009298582,9172.636527208773,9487.027826046044],\"type\":\"bar\",\"xaxis\":\"x8\",\"yaxis\":\"y8\"},{\"name\":\"Array Size _policy\",\"y\":[9642.388315200806,9540.761044391957,9428.39483246044,9328.21667536127,9245.169102433145,9180.132154976985,9157.607754191036,9041.216636582263,8934.984385041545,8834.10456931818,8820.71640441594,8717.55090475541,8252.486133722503,8424.679215056445,8828.456981824611,8534.729344142981,8186.502429908157,8003.341482638109,9256.144666989383,9434.424639527899,9601.16409713631,9154.34238045592,9581.692815749262,8895.42680110169,9647.139095266813],\"type\":\"bar\",\"xaxis\":\"x9\",\"yaxis\":\"y9\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.9185185185185185,1.0],\"title\":{\"text\":\"Value\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.8037037037037038,0.8851851851851853]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,1.0]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.6888888888888889,0.7703703703703704]},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.0,1.0]},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.5740740740740741,0.6555555555555556]},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,1.0]},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.45925925925925926,0.5407407407407407]},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.0,1.0]},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.34444444444444444,0.42592592592592593]},\"xaxis7\":{\"anchor\":\"y7\",\"domain\":[0.0,1.0]},\"yaxis7\":{\"anchor\":\"x7\",\"domain\":[0.22962962962962963,0.3111111111111111]},\"xaxis8\":{\"anchor\":\"y8\",\"domain\":[0.0,1.0]},\"yaxis8\":{\"anchor\":\"x8\",\"domain\":[0.11481481481481481,0.1962962962962963],\"range\":[6000,9495.207013664198]},\"xaxis9\":{\"anchor\":\"y9\",\"domain\":[0.0,1.0]},\"yaxis9\":{\"anchor\":\"x9\",\"domain\":[0.0,0.08148148148148149],\"range\":[6000,9648.139095266813]},\"title\":{\"text\":\"Line and Bar Plots in Separate Subplots\"},\"height\":1400,\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5105154d-d74a-4060-b4f1-95cbaf7b545f');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#bla \n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a figure with subplots (7 rows, 1 column)\n",
    "fig = make_subplots(rows=9, cols=1)\n",
    "\n",
    "# Add line plots for the first 6 arrays in separate rows\n",
    "fig.add_trace(go.Scatter(y=budget_arr, mode='lines', name='Budget'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(y=import_price_arr, mode='lines', name='Import Price'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(y=export_price_arr, mode='lines', name='Export Price'), row=3, col=1)\n",
    "fig.add_trace(go.Scatter(y=PV_panel_price_arr, mode='lines', name='Panel CAPEX'), row=4, col=1)\n",
    "fig.add_trace(go.Scatter(y=efficency_develop_arr, mode='lines', name='Efficency'), row=5, col=1)\n",
    "fig.add_trace(go.Scatter(y=current_interest_arr, mode='lines', name='Interest'), row=6, col=1)\n",
    "fig.add_trace(go.Scatter(y=current_interest_arr_policy, mode='lines', name='Interest'), row=7, col=1)\n",
    "# Add a bar plot for the 7th array in the last row\n",
    "fig.add_trace(go.Bar(y=pv_array_size_arr, name='Array Size _agent'), row=8, col=1)\n",
    "\n",
    "# Add a bar plot for the 7th array in the last row\n",
    "fig.add_trace(go.Bar(y=Ppv_array_size_policy, name='Array Size _policy'), row=9, col=1)\n",
    "\n",
    "# Update layout for clarity\n",
    "fig.update_layout(title=\"Line and Bar Plots in Separate Subplots\",\n",
    "                  xaxis_title=\"Index\",\n",
    "                  yaxis_title=\"Value\",\n",
    "                  height=1400,  # Increase height to accommodate all subplots\n",
    "                  showlegend=True)\n",
    "\n",
    "# Update the y-axis range for the 7th subplot to start from 12\n",
    "fig.update_yaxes(range=[6000, max(pv_array_size_arr) + 1], row=8, col=1)\n",
    "fig.update_yaxes(range=[6000, max(Ppv_array_size_policy) + 1], row=9, col=1)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b3ea5166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array(13, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(2, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(2, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(2, dtype=int64),\n",
       "  array(2, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(2, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(4, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(2, dtype=int64),\n",
       "  array(1, dtype=int64),\n",
       "  array(3, dtype=int64),\n",
       "  array(1, dtype=int64)],\n",
       " [15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 2, 1, 1, 3, 3, 3, 3, 0, 2, 1, 2])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_arr, action_arr_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc2865e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a3fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
